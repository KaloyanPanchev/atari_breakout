{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import retro\n",
    "import gym\n",
    "import os\n",
    "\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of frame:  Box(210, 160, 3)\n",
      "number of actions:  4\n",
      "actions:  ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "action_size = env.env.action_space.n\n",
    "possible_actions = env.unwrapped.get_action_meanings()\n",
    "\n",
    "print(\"size of frame: \", env.observation_space)\n",
    "print(\"number of actions: \", action_size)\n",
    "print(\"actions: \", possible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 4\n",
      "0.0 False 4\n",
      "0.0 False 4\n",
      "0.0 False 4\n",
      "0.0 False 4\n",
      "0.0 False 4\n"
     ]
    }
   ],
   "source": [
    "#try a few actions and games:\n",
    "frame = env.reset()\n",
    "for i in range(30):\n",
    "    new_frame, reward, terminal, info = env.step(1)\n",
    "    print(reward, terminal, info['ale.lives'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameProcessor(object):\n",
    "    def __init__(self, height=84, width = 84):\n",
    "        self.height = height\n",
    "        self.widht = width\n",
    "        self.frame = tf.placeholder(shape=[210,160,3], dtype=tf.uint8)\n",
    "        \n",
    "        self.gray_scaled = tf.image.rgb_to_grayscale(frame)\n",
    "        self.cropped_frame = tf.image.crop_to_bounding_box(self.gray_scaled,34, 0, 160, 160)\n",
    "        self.processed = tf.image.resize_images(self.cropped_frame, [84, 84] , method = tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    def __call__(self, session, frame):\n",
    "        return session.run(self.processed, feed_dict={self.frame:frame})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_four_frames(frame_number, frames_for_stack, reward, path):\n",
    "    for idx, frame_idx in enumerate(frames_for_stack):\n",
    "        frames_for_stack[idx] = resize(frame_idx, (420, 320, 3), preserve_range=True, order=0).astype(np.uint8)\n",
    "    \n",
    "    imageio.mimsave(f'{path}{\"Breakout_frame_{0}_reward_{1}.gif\".format(frame_number,reward)}', frames_for_stack, duration =1/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(object):\n",
    "    def __init__(self, number_actions, hidden=512, learning_rate=0.0000625, height=84, width=84, history_length=4):\n",
    "        self.number_actions = number_actions\n",
    "        self.hidden = hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.history_length = history_length\n",
    "        \n",
    "        self.input = tf.placeholder(shape=[None, self.height, self.width, self.history_length], dtype=tf.float32)\n",
    "        self.input = self.input/255\n",
    "        \n",
    "        #CONV LAYERS:\n",
    "        \n",
    "        #CONV 1:\n",
    "        self.conv1 = tf.layers.conv2d(inputs=self.input, filters=32, kernel_size=[8, 8], strides=4,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                     padding=\"valid\", activation=tf.nn.elu, use_bias=False, name=\"conv1\")\n",
    "        \n",
    "        #CONV 2:\n",
    "        self.conv2 = tf.layers.conv2d(inputs=self.conv1, filters=64, kernel_size=[4, 4], strides=2,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                     padding=\"valid\", activation=tf.nn.elu, use_bias=False, name=\"conv2\")\n",
    "        \n",
    "        #CONV 3:\n",
    "        self.conv3 = tf.layers.conv2d(inputs=self.conv2, filters=64, kernel_size=[3, 3], strides=1,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                     padding=\"valid\", activation=tf.nn.elu, use_bias=False, name=\"conv3\")\n",
    "        \n",
    "        self.flatten = tf.layers.flatten(self.conv3)\n",
    "        \n",
    "        \n",
    "        #Calculate V(s)\n",
    "        self.value_fc = tf.layers.dense(inputs = self.flatten, units=hidden, activation=tf.nn.elu,\n",
    "                                        kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"value_fc\")\n",
    "        \n",
    "        self.value = tf.layers.dense(inputs = self.value_fc, units=1, activation=None,\n",
    "                                        kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"value\")\n",
    "        \n",
    "        #Calculate A(s,a)\n",
    "        self.advantage_fc = tf.layers.dense(inputs = self.flatten, units=hidden, activation=tf.nn.elu,\n",
    "                                        kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"advantage_fc\")\n",
    "        \n",
    "        self.advantage = tf.layers.dense(inputs = self.advantage_fc, units=self.number_actions, activation=None,\n",
    "                                        kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"advantages\")\n",
    "\n",
    "        \n",
    "        #Combine the two:\n",
    "        self.q_values = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "        self.best_action = tf.argmax(self.q_values, 1)\n",
    "        \n",
    "        #target q:\n",
    "        self.target_q = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        \n",
    "        #action we took:\n",
    "        self.action = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        \n",
    "        #Q value of the action above:\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.q_values, tf.one_hot(self.action, self.number_actions, dtype=tf.float32))\n",
    "                               , axis=1)\n",
    "        \n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.losses.huber_loss(labels=self.target_q, predictions= self.Q))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.update = self.optimizer.minimize(self.loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(session, DDQN, explore_start, explore_stop, decay_rate, decay_step, state, action_size):\n",
    "    exp_tradeoff = np.random.rand()\n",
    "    \n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if(explore_probability > exp_tradeoff):\n",
    "        #random action(exploration)\n",
    "        action = np.random.randint(0, action_size)\n",
    "    else:\n",
    "        action = session.run(DDQN.best_action, feed_dict={DDQN.input:[state]})[0]\n",
    "        \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReplayMemory(object):\n",
    "    def __init__(self, size= 100000, height=84, width= 84, history_length=4, batch_size=32):\n",
    "        self.size = size\n",
    "        self.hight = height\n",
    "        self.history_length = history_length\n",
    "        self.batch_size = batch_size\n",
    "        self.count = 0\n",
    "        self.current= 0\n",
    "        \n",
    "        #where the transitions will be stored:\n",
    "        self.actions = np.empty(self.size, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size, dtype=np.int32)\n",
    "        self.frames = np.empty((self.size, self.height, self.width), dtype=np.uint8)\n",
    "        self.terminal_flags = np.empty(self.size, dtype=np.int32)\n",
    "        \n",
    "        #memory for the minibatch:\n",
    "        self.states = np.empty((self.batch_size, self.history_length, self.frame_height, self.frame_width),\n",
    "                               dtype=np.uint8)\n",
    "        self.new_states = np.empty((self.batch_size, self.history_length, self.frame_height, self.frame_width),\n",
    "                               dtype=np.uint8)\n",
    "        self.indices = np.empty(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "        def add_experience(self, action, frame, reward, terminal):\n",
    "            if frame.shape != (self.height, self.width):\n",
    "                raise ValueError(\"Dimensions of frame do not match 84x84\")\n",
    "            \n",
    "            #add the experience:\n",
    "            self.actions[self.current] = action\n",
    "            self.frames[self.current, ...] = frame\n",
    "            self.rewards[self.current] = reward\n",
    "            self.terminal_flags[self.current] = terminal\n",
    "            self.count = max(self.count, self.current+1)\n",
    "            self.current = (self.current + 1) % self.size # if we reach the limit we start overriding the first ones\n",
    "        \n",
    "        def _get_state(self, index):\n",
    "            if self.count is 0:\n",
    "                raise ValueError(\"the memory is empty\")\n",
    "            if index < 3:\n",
    "                raise ValueError(\"index must be at least 3\")\n",
    "            return self.frames[index-self.history_length+1:index+1, ...] #get the 4 frames that represent this state\n",
    "            \n",
    "        def _get_valid_indices(self):\n",
    "            for i in range(self.batch_size):\n",
    "                while True:\n",
    "                    index = random.randint(self.history_length, self.count - 1)\n",
    "                    if index < self.history_length: # index cannot be smalled than 4\n",
    "                        continue\n",
    "                    if index >= self.current and index - self.history_length <= self.current: # there should be atleast 4 frames to get after the state\n",
    "                        continue\n",
    "                    if self.terminal_flags[index - self.history_length:index].any(): #if there is a terminal flag active, that means that in those four frame the agent died => we do not want to take them as a state\n",
    "                        continue\n",
    "                    break\n",
    "                self.indices[i] = index\n",
    "        \n",
    "        def get_minibatch(self):\n",
    "            \n",
    "            if self.count < self.history_length:\n",
    "                raise ValueError(\"not enough memories to get a minibatch\")\n",
    "                \n",
    "            self._get_valid_indices()\n",
    "            \n",
    "            for i, idx in enumerate(self.indices):\n",
    "                self.states[i] = self._get_state(idx - 1)\n",
    "                self.new_states[i] = self._get_state(idx)\n",
    "                \n",
    "            return np.transpose(self.states, axes=(0,2,3,1)), self.actions[self.indices], self.rewards[self.indices], self.transpose(self.new_states, axes(0,2,3,1)), self.terminal_flags[self.indices]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(session, replay_memory, main_ddqn, target_ddqn, batch_size, gamma):\n",
    "    states, actions, rewards, new_states, terminal_flags = replay_memory.get_minibatch()\n",
    "    \n",
    "    best_new_actions = session.run(main_ddqn.best_action, feed_dict={main.ddqn.input:new_states})\n",
    "    \n",
    "    q_vals = session.run(target_ddqn.q_values, feed_dict={targer_ddqn.input:new_states})\n",
    "    double_q = q_vals[range(batch_size), best_new_actions]\n",
    "    \n",
    "    target_q = rewards + (gamma*double_q*(1 - terminal_flags))\n",
    "    \n",
    "    loss, _= session.run([main_ddqn.loss, main_ddqn.update]\n",
    "                         , feed_dict={main_ddqn.input:states, main_ddqn.target_q:target_q, main_ddqn.action:actions})\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TargetNetworkUpdater(object):\n",
    "    def __init__(self, main_ddqn_vars, target_ddqn_vars):\n",
    "        self.main_ddqn_vars = main_ddqn_vars\n",
    "        self.target_ddqn_vars = target_ddqn_vars\n",
    "        \n",
    "    def _update_target_vars(self):\n",
    "        update_ops = []\n",
    "        for i, var in enumerate(self.main_ddqn_vars):\n",
    "            copy_op = self.target_ddqn_vars[i].assign(var.value())\n",
    "            update_ops.append(copy_op)\n",
    "        return update_ops\n",
    "    \n",
    "    def __call__(self, sess):\n",
    "        update_ops = self._update_target_vars()\n",
    "        for copy_op in update_ops:\n",
    "            sess.run(copy_op)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreakoutGame(object):\n",
    "    \n",
    "    def __init__(self, env_name=\"BreakoutDeterministic-v4\", no_op_steps=10, history_length=4):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.process_frame = FrameProcessor()\n",
    "        self.state = None\n",
    "        self.last_lives = 0\n",
    "        self.no_op_steps = no_op_steps\n",
    "        self.history_length = history_length\n",
    "        \n",
    "    def reset(self, sess, evaluation=False):\n",
    "        frame = self.env.reset()\n",
    "        self.last_lives = 0\n",
    "        terminal_life_lost = True\n",
    "        \n",
    "        if evaluation:\n",
    "            for _ in range(random.randint(1, self.no_op_steps)):\n",
    "                frame, _, _, _ = self.env.step(1) #fire the ball\n",
    "                \n",
    "        processed_frame = self.process_frame(sess, frame)\n",
    "        self.state = np.repeat(processed_frame, history_length, axis=2)\n",
    "        \n",
    "        return terminal_life_lost\n",
    "    \n",
    "    def step(self, sess, action):\n",
    "        \n",
    "        new_frame, reward, terminal, info = self.env.step(action)\n",
    "        \n",
    "        if info['ale.lives'] < self.last_lives:\n",
    "            terminal_life_lost = True\n",
    "        else:\n",
    "            terminal_life_lost = terminal\n",
    "        self.last_lives = info['ale.lives']\n",
    "        \n",
    "        processed_new_frame = self.process_frame(sess, new_frame)\n",
    "        new_state = np.append(self.state[:, :, 1:], processed_new_frame, axis=2)\n",
    "        self.state = new_state\n",
    "        \n",
    "        return processed_new_frame, reward, terminal, terminal_life_lost, new_frame\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip the rewards to be -1, 0, 1. Proved to be benefitial for Breakout\n",
    "def clip_reward(reward):\n",
    "    if reward > 0:\n",
    "        return 1\n",
    "    elif reward == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the hyper-params:\n",
    "\n",
    "MAX_EPISODE_LENGTH = 1800\n",
    "EVAL_FREQUENCE = 200000\n",
    "EVAL_STEPS = 10000\n",
    "TARGET_NETWORK_UPDATE_FREQ = 10000\n",
    "\n",
    "DISCOUNT_FACTOR = 0.99 # Gamma in the Q learning equation\n",
    "REPLAY_MEMORY_START_SIZE = 50000 # random actions at the begining\n",
    "\n",
    "MAX_FRAMES = 7000000\n",
    "MEMORY_SIZE = 1000000\n",
    "NO_OP_STEPS = 10\n",
    "\n",
    "UPDATE_FREQ = 4\n",
    "HIDDEN = 512\n",
    "\n",
    "LEARNING_RATE = 0.0000625\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "PATH = \"output/\"\n",
    "INFO = \"info\"\n",
    "RUNID = \"run1\"\n",
    "\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "os.makedirs(os.path.join(INFO, RUNID), exist_ok=True)\n",
    "SUMM_WRITER = tf.summary.FileWriter(os.path.join(INFO, RUNID))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kaloyan/deeplearning/final_project/final_project/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-6-7c1f8e3b3f40>:18: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-7c1f8e3b3f40>:30: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-7c1f8e3b3f40>:35: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/kaloyan/deeplearning/final_project/final_project/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:448: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/kaloyan/deeplearning/final_project/final_project/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "atari = BreakoutGame()\n",
    "\n",
    "#create main DQN:\n",
    "with tf.variable_scope('mainDDQN'):\n",
    "    MAIN_DDQN = DDQN(atari.env.action_space.n, HIDDEN, LEARNING_RATE)\n",
    "\n",
    "MAIN_DDQN_VARS = tf.trainable_variables(scope='mainDDQN')\n",
    "\n",
    "\n",
    "#create target DQN:\n",
    "with tf.variable_scope('targetDDQN'):\n",
    "    MAIN_DDQN = DDQN(atari.env.action_space.n, HIDDEN)\n",
    "\n",
    "TARGET_DDQN_VARS = tf.trainable_variables(scope='targetDDQN')\n",
    "\n",
    "\n",
    "init= tf.global_variables_initializer()\n",
    "saved = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS_IDS = [\"conv1\", \"conv2\", \"conv3\", \"value_fc\", \"value\", \"advantage_fc\", \"advantages\"]\n",
    "\n",
    "with tf.name_scope(\"Performace\"):\n",
    "    LOSS_PH = tf.placeholder(tf.float32, shape=None, name=\"loss_summary\")\n",
    "    LOSS_SUMMARY = tf.summary.scalar(\"loss\", LOSS_PH)\n",
    "    REWARD_PH = tf.placeholder(tf.float32, shape=None, name=\"reward_summary\")\n",
    "    REWARD_SUMMARY = tf.summary.scalar(\"reward\", REWARD_PH)\n",
    "    EVAL_SCORE__PH = tf.placeholder(tf.float32, shape=None, name=\"eval_summary\")\n",
    "    EVAL_SCORE__SUMMARY = tf.summary.scalar(\"eval_score\", EVAL_SCORE__PH)\n",
    "    \n",
    "PERFORMANCE_SUMMARIES = tf.summary.merge([LOSS_SUMMARY, REWARD_SUMMARY])\n",
    "\n",
    "with tf.name_scope(\"Parameters\"):\n",
    "    ALL_PARAM_SUMMARIES = []\n",
    "    for i, Id in enumerate(LAYERS_IDS):\n",
    "        with tf.name_scope(\"mainDDQN/\"):\n",
    "            MAIN_DDQN_KERNEL = tf.summary.histogram(Id, tf.reshape(MAIN_DDQN_VARS[i], shape=[-1]))\n",
    "        ALL_PARAM_SUMMARIES.extend([MAIN_DDQN_KERNEL])\n",
    "PARAM_SUMMARIES = tf.summary.merge(ALL_PARAM_SUMMARIES)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
