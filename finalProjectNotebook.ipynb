{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import retro\n",
    "import gym\n",
    "import os\n",
    "\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "#remove deprication msgs for .layers\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of frame:  Box(210, 160, 3)\n",
      "number of actions:  4\n",
      "actions:  ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "ACTION_SIZE = env.env.action_space.n\n",
    "possible_actions = env.unwrapped.get_action_meanings()\n",
    "\n",
    "print(\"size of frame: \", env.observation_space)\n",
    "print(\"number of actions: \", ACTION_SIZE)\n",
    "print(\"actions: \", possible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 4\n",
      "0.0 False 4\n",
      "0.0 False 4\n",
      "0.0 False 4\n",
      "0.0 False 4\n",
      "0.0 False 4\n"
     ]
    }
   ],
   "source": [
    "#try a few actions and games:\n",
    "frame = env.reset()\n",
    "for i in range(30):\n",
    "    new_frame, reward, terminal, info = env.step(1)\n",
    "    print(reward, terminal, info['ale.lives'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameProcessor(object):\n",
    "    def __init__(self, height=84, width = 84):\n",
    "        self.height = height\n",
    "        self.widht = width\n",
    "        self.frame = tf.placeholder(shape=[210,160,3], dtype=tf.uint8)\n",
    "        \n",
    "        self.gray_scaled = tf.image.rgb_to_grayscale(frame)\n",
    "        self.cropped_frame = tf.image.crop_to_bounding_box(self.gray_scaled,34, 0, 160, 160)\n",
    "        self.processed = tf.image.resize_images(self.cropped_frame, [84, 84] , method = tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    def __call__(self, session, frame):\n",
    "        return session.run(self.processed, feed_dict={self.frame:frame})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(object):\n",
    "    def __init__(self, number_actions, hidden=512, learning_rate=0.0000625, height=84, width=84, history_length=4):\n",
    "        self.number_actions = number_actions\n",
    "        self.hidden = hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.history_length = history_length\n",
    "        \n",
    "        self.input = tf.placeholder(shape=[None, self.height, self.width, self.history_length], dtype=tf.float32)\n",
    "        self.input = self.input/255\n",
    "        \n",
    "        #CONV LAYERS:\n",
    "        \n",
    "        #CONV 1:\n",
    "        self.conv1 = tf.layers.conv2d(inputs=self.input, filters=32, kernel_size=[8, 8], strides=4,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                     padding=\"valid\", activation=tf.nn.elu, use_bias=False, name=\"conv1\")\n",
    "        \n",
    "        #CONV 2:\n",
    "        self.conv2 = tf.layers.conv2d(inputs=self.conv1, filters=64, kernel_size=[4, 4], strides=2,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                     padding=\"valid\", activation=tf.nn.elu, use_bias=False, name=\"conv2\")\n",
    "        \n",
    "        #CONV 3:\n",
    "        self.conv3 = tf.layers.conv2d(inputs=self.conv2, filters=64, kernel_size=[3, 3], strides=1,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                     padding=\"valid\", activation=tf.nn.elu, use_bias=False, name=\"conv3\")\n",
    "        \n",
    "        self.flatten = tf.layers.flatten(self.conv3)\n",
    "        \n",
    "        \n",
    "        #Calculate V(s)\n",
    "        self.value_fc = tf.layers.dense(inputs = self.flatten, units=hidden, activation=tf.nn.elu,\n",
    "                                        kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"value_fc\")\n",
    "        \n",
    "        self.value = tf.layers.dense(inputs = self.value_fc, units=1, activation=None,\n",
    "                                        kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"value\")\n",
    "        \n",
    "        #Calculate A(s,a)\n",
    "        self.advantage_fc = tf.layers.dense(inputs = self.flatten, units=hidden, activation=tf.nn.elu,\n",
    "                                        kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"advantage_fc\")\n",
    "        \n",
    "        self.advantage = tf.layers.dense(inputs = self.advantage_fc, units=self.number_actions, activation=None,\n",
    "                                        kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"advantages\")\n",
    "\n",
    "        \n",
    "        #Combine the two:\n",
    "        self.q_values = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "        self.best_action = tf.argmax(self.q_values, 1)\n",
    "        \n",
    "        #target q:\n",
    "        self.target_q = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        \n",
    "        #action we took:\n",
    "        self.action = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        \n",
    "        #Q value of the action above:\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.q_values, tf.one_hot(self.action, self.number_actions, dtype=tf.float32))\n",
    "                               , axis=1)\n",
    "        \n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.losses.huber_loss(labels=self.target_q, predictions= self.Q))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.update = self.optimizer.minimize(self.loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(session, DDQN, explore_start, explore_stop, decay_rate, decay_step, state, action_size, evaluation = False):\n",
    "    exp_tradeoff = np.random.rand()\n",
    "    \n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if evaluation:\n",
    "        explore_probability = 0.0\n",
    "    \n",
    "    if(explore_probability > exp_tradeoff):\n",
    "        #random action(exploration)\n",
    "        action = np.random.randint(0, action_size)\n",
    "    else:\n",
    "        action = session.run(DDQN.best_action, feed_dict={DDQN.input:[state]})[0]\n",
    "        \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, size=100000, height=84, width= 84, history_length=4, batch_size=32):\n",
    "        self.size = size\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.history_length = history_length\n",
    "        self.batch_size = batch_size\n",
    "        self.count = 0\n",
    "        self.current= 0\n",
    "        \n",
    "        #where the transitions will be stored:\n",
    "        self.actions = np.empty(self.size, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size, dtype=np.int32)\n",
    "        self.frames = np.empty((self.size, self.height, self.width), dtype=np.uint8)\n",
    "        self.terminal_flags = np.empty(self.size, dtype=np.int32)\n",
    "        \n",
    "        #memory for the minibatch:\n",
    "        self.states = np.empty((self.batch_size, self.history_length, self.height, self.width),\n",
    "                               dtype=np.uint8)\n",
    "        self.new_states = np.empty((self.batch_size, self.history_length, self.height, self.width),\n",
    "                               dtype=np.uint8)\n",
    "        self.indices = np.empty(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "    def add_experience(self, action, frame, reward, terminal):\n",
    "        if frame.shape != (self.height, self.width):\n",
    "            raise ValueError(\"Dimensions of frame do not match 84x84\")\n",
    "            \n",
    "        #add the experience:\n",
    "        self.actions[self.current] = action\n",
    "        self.frames[self.current, ...] = frame\n",
    "        self.rewards[self.current] = reward\n",
    "        self.terminal_flags[self.current] = terminal\n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current + 1) % self.size # if we reach the limit we start overriding the first ones\n",
    "        \n",
    "    def _get_state(self, index):\n",
    "        if self.count is 0:\n",
    "            raise ValueError(\"the memory is empty\")\n",
    "        if index < 3:\n",
    "            raise ValueError(\"index must be at least 3\")\n",
    "        return self.frames[index-self.history_length+1:index+1, ...] #get the 4 frames that represent this state\n",
    "            \n",
    "    def _get_valid_indices(self):\n",
    "        for i in range(self.batch_size):\n",
    "            while True:\n",
    "                index = random.randint(self.history_length, self.count - 1)\n",
    "                if index < self.history_length: # index cannot be smalled than 4\n",
    "                    continue\n",
    "                if index >= self.current and index - self.history_length <= self.current: # there should be atleast 4 frames to get after the state\n",
    "                    continue\n",
    "                if self.terminal_flags[index - self.history_length:index].any(): #if there is a terminal flag active, that means that in those four frame the agent died => we do not want to take them as a state\n",
    "                    continue\n",
    "                break\n",
    "            self.indices[i] = index\n",
    "        \n",
    "    def get_minibatch(self):\n",
    "            \n",
    "        if self.count < self.history_length:\n",
    "            raise ValueError(\"not enough memories to get a minibatch\")\n",
    "                \n",
    "        self._get_valid_indices()\n",
    "            \n",
    "        for i, idx in enumerate(self.indices):\n",
    "            self.states[i] = self._get_state(idx - 1)\n",
    "            self.new_states[i] = self._get_state(idx)\n",
    "                \n",
    "        return np.transpose(self.states, axes=(0,2,3,1)), self.actions[self.indices], self.rewards[self.indices], np.transpose(self.new_states, axes=(0,2,3,1)), self.terminal_flags[self.indices]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(session, replay_memory, main_ddqn, target_ddqn, batch_size, gamma):\n",
    "    states, actions, rewards, new_states, terminal_flags = replay_memory.get_minibatch()\n",
    "    \n",
    "    best_new_actions = session.run(main_ddqn.best_action, feed_dict={main_ddqn.input:new_states})\n",
    "    \n",
    "    q_vals = session.run(target_ddqn.q_values, feed_dict={target_ddqn.input:new_states})\n",
    "    double_q = q_vals[range(batch_size), best_new_actions]\n",
    "    \n",
    "    target_q = rewards + (gamma*double_q*(1 - terminal_flags))\n",
    "    \n",
    "    loss, _= session.run([main_ddqn.loss, main_ddqn.update]\n",
    "                         , feed_dict={main_ddqn.input:states, main_ddqn.target_q:target_q, main_ddqn.action:actions})\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetNetworkUpdater(object):\n",
    "    def __init__(self, main_ddqn_vars, target_ddqn_vars):\n",
    "        self.main_ddqn_vars = main_ddqn_vars\n",
    "        self.target_ddqn_vars = target_ddqn_vars\n",
    "        \n",
    "    def _update_target_vars(self):\n",
    "        update_ops = []\n",
    "        for i, var in enumerate(self.main_ddqn_vars):\n",
    "            copy_op = self.target_ddqn_vars[i].assign(var.value())\n",
    "            update_ops.append(copy_op)\n",
    "        return update_ops\n",
    "    \n",
    "    def __call__(self, sess):\n",
    "        update_ops = self._update_target_vars()\n",
    "        for copy_op in update_ops:\n",
    "            sess.run(copy_op)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreakoutGame(object):\n",
    "    \n",
    "    def __init__(self, env_name=\"BreakoutDeterministic-v4\", no_op_steps=10, history_length=4):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.process_frame = FrameProcessor()\n",
    "        self.state = None\n",
    "        self.last_lives = 0\n",
    "        self.no_op_steps = no_op_steps\n",
    "        self.history_length = history_length\n",
    "        \n",
    "    def reset(self, sess, evaluation=False):\n",
    "        frame = self.env.reset()\n",
    "        self.last_lives = 0\n",
    "        terminal_life_lost = True\n",
    "        \n",
    "        if evaluation:\n",
    "            for _ in range(random.randint(1, self.no_op_steps)):\n",
    "                frame, _, _, _ = self.env.step(1) #fire the ball\n",
    "                \n",
    "        processed_frame = self.process_frame(sess, frame)\n",
    "        self.state = np.repeat(processed_frame, self.history_length, axis=2)\n",
    "        \n",
    "        return terminal_life_lost\n",
    "    \n",
    "    def step(self, sess, action):\n",
    "        \n",
    "        new_frame, reward, terminal, info = self.env.step(action)\n",
    "        \n",
    "        if info['ale.lives'] < self.last_lives:\n",
    "            terminal_life_lost = True\n",
    "        else:\n",
    "            terminal_life_lost = terminal\n",
    "        self.last_lives = info['ale.lives']\n",
    "        \n",
    "        processed_new_frame = self.process_frame(sess, new_frame)\n",
    "        new_state = np.append(self.state[:, :, 1:], processed_new_frame, axis=2)\n",
    "        self.state = new_state\n",
    "        \n",
    "        return processed_new_frame, reward, terminal, terminal_life_lost, new_frame\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip the rewards to be -1, 0, 1. Proved to be benefitial for Breakout\n",
    "def clip_reward(reward):\n",
    "    if reward > 0:\n",
    "        return 1\n",
    "    elif reward == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#define the hyper-params:\n",
    "\n",
    "MAX_EPISODE_LENGTH = 18000\n",
    "EVAL_FREQUENCY = 200000\n",
    "EVAL_STEPS = 10000\n",
    "TARGET_NETWORK_UPDATE_FREQ = 10000\n",
    "\n",
    "DISCOUNT_FACTOR = 0.95 # Gamma in the Q learning equation\n",
    "REPLAY_MEMORY_START_SIZE = 50000 # random actions at the begining\n",
    "\n",
    "MAX_FRAMES = 7000000\n",
    "MEMORY_SIZE = 1000000\n",
    "NO_OP_STEPS = 10\n",
    "\n",
    "UPDATE_FREQ = 4\n",
    "HIDDEN = 512\n",
    "\n",
    "LEARNING_RATE = 0.0000625\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "EXPLORE_START = 1.0\n",
    "EXPLORE_STOP = 0.01\n",
    "DECAY_RATE = 0.00005\n",
    "\n",
    "#IMPORTANT FOR TRAINING OR TESTING:\n",
    "PATH = \"output/\"\n",
    "INFO = \"info\"\n",
    "RUNID = \"run1\"\n",
    "PATH = PATH + RUNID \n",
    "TRAIN = True\n",
    "\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "os.makedirs(os.path.join(INFO, RUNID), exist_ok=True)\n",
    "SUMM_WRITER = tf.summary.FileWriter(os.path.join(INFO, RUNID))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "atari = BreakoutGame()\n",
    "\n",
    "#create main DQN:\n",
    "with tf.variable_scope('mainDDQN'):\n",
    "    MAIN_DDQN = DDQN(atari.env.action_space.n, HIDDEN, LEARNING_RATE)\n",
    "\n",
    "MAIN_DDQN_VARS = tf.trainable_variables(scope='mainDDQN')\n",
    "\n",
    "\n",
    "#create target DQN:\n",
    "with tf.variable_scope('targetDDQN'):\n",
    "    TARGET_DDQN = DDQN(atari.env.action_space.n, HIDDEN)\n",
    "\n",
    "TARGET_DDQN_VARS = tf.trainable_variables(scope='targetDDQN')\n",
    "\n",
    "\n",
    "init= tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS_IDS = [\"conv1\", \"conv2\", \"conv3\", \"value_fc\", \"value\", \"advantage_fc\", \"advantages\"]\n",
    "\n",
    "with tf.name_scope(\"Performace\"):\n",
    "    LOSS_PH = tf.placeholder(tf.float32, shape=None, name=\"loss_summary\")\n",
    "    LOSS_SUMMARY = tf.summary.scalar(\"loss\", LOSS_PH)\n",
    "    REWARD_PH = tf.placeholder(tf.float32, shape=None, name=\"reward_summary\")\n",
    "    REWARD_SUMMARY = tf.summary.scalar(\"reward\", REWARD_PH)\n",
    "    EVAL_SCORE_PH = tf.placeholder(tf.float32, shape=None, name=\"eval_summary\")\n",
    "    EVAL_SCORE_SUMMARY = tf.summary.scalar(\"eval_score\", EVAL_SCORE_PH)\n",
    "    \n",
    "PERFORMANCE_SUMMARIES = tf.summary.merge([LOSS_SUMMARY, REWARD_SUMMARY])\n",
    "\n",
    "with tf.name_scope(\"Parameters\"):\n",
    "    ALL_PARAM_SUMMARIES = []\n",
    "    for i, Id in enumerate(LAYERS_IDS):\n",
    "        with tf.name_scope(\"mainDDQN/\"):\n",
    "            MAIN_DDQN_KERNEL = tf.summary.histogram(Id, tf.reshape(MAIN_DDQN_VARS[i], shape=[-1]))\n",
    "        ALL_PARAM_SUMMARIES.extend([MAIN_DDQN_KERNEL])\n",
    "PARAM_SUMMARIES = tf.summary.merge(ALL_PARAM_SUMMARIES)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    my_replay_memory = ReplayMemory(size=MEMORY_SIZE, batch_size=BATCH_SIZE)\n",
    "    update_networks = TargetNetworkUpdater(MAIN_DDQN_VARS, TARGET_DDQN_VARS)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        frame_number = 0\n",
    "        rewards = []\n",
    "        loss_list = []\n",
    "        \n",
    "        decay_step = 0\n",
    "        tau = 0\n",
    "        \n",
    "        while frame_number < MAX_FRAMES:\n",
    "            print(frame_number)\n",
    "        #training:\n",
    "            epoch_frame = 0\n",
    "            while epoch_frame < EVAL_FREQUENCY:\n",
    "                terminal_life_lost = atari.reset(sess)\n",
    "                episode_reward_sum = 0\n",
    "                for _ in range(MAX_EPISODE_LENGTH):\n",
    "                    \n",
    "                    #take the action:\n",
    "                    action, explore_prob = predict_action(sess, MAIN_DDQN, EXPLORE_START, EXPLORE_STOP,\n",
    "                                                          DECAY_RATE, decay_step, atari.state, action_size=ACTION_SIZE)\n",
    "                    decay_step += 1\n",
    "                    \n",
    "                    processed_new_frame, reward, terminal, terminal_life_lost, _ = atari.step(sess, action)\n",
    "                    \n",
    "                    frame_number += 1\n",
    "                    epoch_frame += 1\n",
    "                    episode_reward_sum += reward\n",
    "                    \n",
    "                    clipped_reward = clip_reward(reward)\n",
    "                    \n",
    "                    my_replay_memory.add_experience(action=action, frame=processed_new_frame[:, :, 0], reward=clipped_reward,\n",
    "                                                   terminal=terminal_life_lost)\n",
    "                    \n",
    "                    if frame_number % UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        loss = learn(sess, my_replay_memory, MAIN_DDQN, TARGET_DDQN, BATCH_SIZE, gamma = DISCOUNT_FACTOR)\n",
    "                        loss_list.append(loss)\n",
    "                    \n",
    "                    if frame_number % TARGET_NETWORK_UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        update_networks(sess)\n",
    "                        \n",
    "                    if terminal:\n",
    "                        terminal = False\n",
    "                        break\n",
    "                \n",
    "                rewards.append(episode_reward_sum)\n",
    "                \n",
    "                if len(rewards) % 10 == 0:\n",
    "                    \n",
    "                    if frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        summ = sess.run(PERFORMANCE_SUMMARIES, feed_dict={LOSS_PH:np.mean(loss_list), REWARD_PH:np.mean(rewards[-10:])})\n",
    "                        \n",
    "                        SUMM_WRITER.add_summary(summ, frame_number)\n",
    "                        loss_list = []\n",
    "                        \n",
    "                    summ_param = sess.run(PARAM_SUMMARIES)\n",
    "                    SUMM_WRITER.add_summary(summ_param, frame_number)\n",
    "                    \n",
    "                    print(\"Number of episodes: {}\".format(len(rewards)), \"Frame number: {}\".format(frame_number),\n",
    "                          \"Rewards mean for the last 10 episodes: {}\".format(np.mean(rewards[-10:])))\n",
    "                    with open(\"rewards.dat\", \"a\") as reward_file:\n",
    "                        print(len(rewards), frame_number, np.mean(rewards[-10:]), file=reward_file)\n",
    "            \n",
    "            #evaluation:\n",
    "            \n",
    "            terminal = True\n",
    "            eval_rewards = []\n",
    "            evaluate_frame_number = 0\n",
    "            \n",
    "            for _ in range(EVAL_STEPS):\n",
    "                if terminal:\n",
    "                    terminal_life_lost = atari.reset(sess, evaluation=True)\n",
    "                    episode_reward_sum = 0\n",
    "                    terminal = False\n",
    "                    \n",
    "                if terminal_life_lost:\n",
    "                    action = 1 \n",
    "                else: \n",
    "                    action, _ = predict_action(sess, MAIN_DDQN, EXPLORE_START, EXPLORE_STOP,\n",
    "                                                                         DECAY_RATE, decay_step, atari.state, ACTION_SIZE, evaluation=True)\n",
    "                \n",
    "                processed_new_frame, reward, terminal, terminal_life_lost, _ = atari.step(sess, action)\n",
    "                evaluate_frame_number += 1\n",
    "                episode_reward_sum += reward\n",
    "                \n",
    "                if terminal:\n",
    "                    eval_rewards.append(episode_reward_sum)\n",
    "                \n",
    "            print(\"Evaluation score:\\n\", np.mean(eval_rewards))\n",
    "            \n",
    "            #save network params:\n",
    "            saver.save(sess, PATH+\"/my_model\", global_step=frame_number)\n",
    "            \n",
    "            summ = sess.run(EVAL_SCORE_SUMMARY, feed_dict={EVAL_SCORE_PH:np.mean(eval_rewards)})\n",
    "            SUMM_WRITER.add_summary(summ, frame_number)\n",
    "            \n",
    "            with open(\"rewardsEval.dat\", \"a\") as eval_reward_file:\n",
    "                 print(frame_number, np.mean(eval_rewards), file=eval_reward_file)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Number of episodes: 10 Frame number: 1951 Rewards mean for the last 10 episodes: 1.3\n",
      "Number of episodes: 20 Frame number: 3673 Rewards mean for the last 10 episodes: 0.9\n",
      "Number of episodes: 30 Frame number: 5639 Rewards mean for the last 10 episodes: 1.3\n",
      "Number of episodes: 40 Frame number: 7422 Rewards mean for the last 10 episodes: 1.0\n",
      "Number of episodes: 50 Frame number: 9065 Rewards mean for the last 10 episodes: 0.6\n",
      "Number of episodes: 60 Frame number: 10813 Rewards mean for the last 10 episodes: 0.8\n",
      "Number of episodes: 70 Frame number: 12766 Rewards mean for the last 10 episodes: 1.2\n",
      "Number of episodes: 80 Frame number: 14638 Rewards mean for the last 10 episodes: 1.0\n",
      "Number of episodes: 90 Frame number: 16545 Rewards mean for the last 10 episodes: 0.9\n",
      "Number of episodes: 100 Frame number: 18249 Rewards mean for the last 10 episodes: 0.4\n",
      "Number of episodes: 110 Frame number: 19996 Rewards mean for the last 10 episodes: 0.5\n",
      "Number of episodes: 120 Frame number: 22029 Rewards mean for the last 10 episodes: 0.6\n",
      "Number of episodes: 130 Frame number: 24030 Rewards mean for the last 10 episodes: 0.6\n",
      "Number of episodes: 140 Frame number: 26091 Rewards mean for the last 10 episodes: 0.8\n",
      "Number of episodes: 150 Frame number: 28182 Rewards mean for the last 10 episodes: 0.5\n",
      "Number of episodes: 160 Frame number: 30551 Rewards mean for the last 10 episodes: 0.7\n",
      "Number of episodes: 170 Frame number: 33052 Rewards mean for the last 10 episodes: 0.7\n",
      "Number of episodes: 180 Frame number: 35180 Rewards mean for the last 10 episodes: 0.1\n",
      "Number of episodes: 190 Frame number: 37520 Rewards mean for the last 10 episodes: 0.2\n",
      "Number of episodes: 200 Frame number: 40055 Rewards mean for the last 10 episodes: 0.4\n",
      "Number of episodes: 210 Frame number: 42804 Rewards mean for the last 10 episodes: 0.2\n",
      "Number of episodes: 220 Frame number: 45948 Rewards mean for the last 10 episodes: 0.2\n",
      "Number of episodes: 230 Frame number: 49221 Rewards mean for the last 10 episodes: 0.7\n",
      "Number of episodes: 240 Frame number: 52054 Rewards mean for the last 10 episodes: 1.4\n",
      "Number of episodes: 250 Frame number: 54861 Rewards mean for the last 10 episodes: 1.7\n",
      "Number of episodes: 260 Frame number: 58143 Rewards mean for the last 10 episodes: 1.3\n",
      "Number of episodes: 270 Frame number: 61859 Rewards mean for the last 10 episodes: 2.7\n",
      "Number of episodes: 280 Frame number: 64849 Rewards mean for the last 10 episodes: 1.3\n",
      "Number of episodes: 290 Frame number: 69373 Rewards mean for the last 10 episodes: 4.0\n",
      "Number of episodes: 300 Frame number: 72106 Rewards mean for the last 10 episodes: 1.8\n",
      "Number of episodes: 310 Frame number: 75286 Rewards mean for the last 10 episodes: 1.6\n",
      "Number of episodes: 320 Frame number: 79192 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 330 Frame number: 82733 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 340 Frame number: 86321 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 350 Frame number: 89836 Rewards mean for the last 10 episodes: 2.7\n",
      "Number of episodes: 360 Frame number: 93049 Rewards mean for the last 10 episodes: 2.2\n",
      "Number of episodes: 370 Frame number: 96237 Rewards mean for the last 10 episodes: 2.0\n",
      "Number of episodes: 380 Frame number: 99437 Rewards mean for the last 10 episodes: 1.7\n",
      "Number of episodes: 390 Frame number: 102857 Rewards mean for the last 10 episodes: 2.2\n",
      "Number of episodes: 400 Frame number: 105997 Rewards mean for the last 10 episodes: 1.9\n",
      "Number of episodes: 410 Frame number: 109448 Rewards mean for the last 10 episodes: 2.0\n",
      "Number of episodes: 420 Frame number: 113040 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 430 Frame number: 116745 Rewards mean for the last 10 episodes: 3.4\n",
      "Number of episodes: 440 Frame number: 120037 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 450 Frame number: 123585 Rewards mean for the last 10 episodes: 2.0\n",
      "Number of episodes: 460 Frame number: 126881 Rewards mean for the last 10 episodes: 1.8\n",
      "Number of episodes: 470 Frame number: 130442 Rewards mean for the last 10 episodes: 2.1\n",
      "Number of episodes: 480 Frame number: 134165 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 490 Frame number: 137821 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 500 Frame number: 141145 Rewards mean for the last 10 episodes: 1.5\n",
      "Number of episodes: 510 Frame number: 144959 Rewards mean for the last 10 episodes: 2.1\n",
      "Number of episodes: 520 Frame number: 148037 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 530 Frame number: 151600 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 540 Frame number: 155309 Rewards mean for the last 10 episodes: 2.9\n",
      "Number of episodes: 550 Frame number: 159048 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 560 Frame number: 162777 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 570 Frame number: 166539 Rewards mean for the last 10 episodes: 2.9\n",
      "Number of episodes: 580 Frame number: 170001 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 590 Frame number: 173199 Rewards mean for the last 10 episodes: 3.1\n",
      "Number of episodes: 600 Frame number: 176902 Rewards mean for the last 10 episodes: 2.1\n",
      "Number of episodes: 610 Frame number: 180601 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 620 Frame number: 184808 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 630 Frame number: 188660 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 640 Frame number: 192857 Rewards mean for the last 10 episodes: 3.6\n",
      "Number of episodes: 650 Frame number: 196569 Rewards mean for the last 10 episodes: 2.0\n",
      "Evaluation score:\n",
      " 0.0\n",
      "200265\n",
      "Number of episodes: 660 Frame number: 200617 Rewards mean for the last 10 episodes: 2.9\n",
      "Number of episodes: 670 Frame number: 204420 Rewards mean for the last 10 episodes: 2.9\n",
      "Number of episodes: 680 Frame number: 209025 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 690 Frame number: 212651 Rewards mean for the last 10 episodes: 2.9\n",
      "Number of episodes: 700 Frame number: 216265 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 710 Frame number: 219829 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 720 Frame number: 223757 Rewards mean for the last 10 episodes: 2.2\n",
      "Number of episodes: 730 Frame number: 227663 Rewards mean for the last 10 episodes: 2.0\n",
      "Number of episodes: 740 Frame number: 231595 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 750 Frame number: 235549 Rewards mean for the last 10 episodes: 1.9\n",
      "Number of episodes: 760 Frame number: 239561 Rewards mean for the last 10 episodes: 3.4\n",
      "Number of episodes: 770 Frame number: 243437 Rewards mean for the last 10 episodes: 2.9\n",
      "Number of episodes: 780 Frame number: 247712 Rewards mean for the last 10 episodes: 3.2\n",
      "Number of episodes: 790 Frame number: 251533 Rewards mean for the last 10 episodes: 2.7\n",
      "Number of episodes: 800 Frame number: 256425 Rewards mean for the last 10 episodes: 3.3\n",
      "Number of episodes: 810 Frame number: 260385 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 820 Frame number: 265541 Rewards mean for the last 10 episodes: 3.6\n",
      "Number of episodes: 830 Frame number: 269575 Rewards mean for the last 10 episodes: 2.9\n",
      "Number of episodes: 840 Frame number: 274358 Rewards mean for the last 10 episodes: 3.5\n",
      "Number of episodes: 850 Frame number: 278792 Rewards mean for the last 10 episodes: 3.3\n",
      "Number of episodes: 860 Frame number: 283025 Rewards mean for the last 10 episodes: 2.9\n",
      "Number of episodes: 870 Frame number: 287673 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 880 Frame number: 291849 Rewards mean for the last 10 episodes: 2.2\n",
      "Number of episodes: 890 Frame number: 295917 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 900 Frame number: 300717 Rewards mean for the last 10 episodes: 3.4\n",
      "Number of episodes: 910 Frame number: 305449 Rewards mean for the last 10 episodes: 3.4\n",
      "Number of episodes: 920 Frame number: 310408 Rewards mean for the last 10 episodes: 3.3\n",
      "Number of episodes: 930 Frame number: 314749 Rewards mean for the last 10 episodes: 2.1\n",
      "Number of episodes: 940 Frame number: 319113 Rewards mean for the last 10 episodes: 2.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes: 950 Frame number: 323629 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 960 Frame number: 327858 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 970 Frame number: 332440 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 980 Frame number: 337245 Rewards mean for the last 10 episodes: 3.1\n",
      "Number of episodes: 990 Frame number: 341658 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 1000 Frame number: 345533 Rewards mean for the last 10 episodes: 1.7\n",
      "Number of episodes: 1010 Frame number: 349643 Rewards mean for the last 10 episodes: 2.2\n",
      "Number of episodes: 1020 Frame number: 353991 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 1030 Frame number: 358418 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 1040 Frame number: 362189 Rewards mean for the last 10 episodes: 2.7\n",
      "Number of episodes: 1050 Frame number: 366561 Rewards mean for the last 10 episodes: 3.2\n",
      "Number of episodes: 1060 Frame number: 370723 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 1070 Frame number: 375113 Rewards mean for the last 10 episodes: 3.3\n",
      "Number of episodes: 1080 Frame number: 379139 Rewards mean for the last 10 episodes: 2.1\n",
      "Number of episodes: 1090 Frame number: 384137 Rewards mean for the last 10 episodes: 3.4\n",
      "Number of episodes: 1100 Frame number: 388445 Rewards mean for the last 10 episodes: 1.9\n",
      "Number of episodes: 1110 Frame number: 393398 Rewards mean for the last 10 episodes: 2.1\n",
      "Number of episodes: 1120 Frame number: 398021 Rewards mean for the last 10 episodes: 3.0\n",
      "Evaluation score:\n",
      " 0.0\n",
      "400578\n",
      "Number of episodes: 1130 Frame number: 402173 Rewards mean for the last 10 episodes: 3.1\n",
      "Number of episodes: 1140 Frame number: 406061 Rewards mean for the last 10 episodes: 1.9\n",
      "Number of episodes: 1150 Frame number: 409883 Rewards mean for the last 10 episodes: 2.2\n",
      "Number of episodes: 1160 Frame number: 414277 Rewards mean for the last 10 episodes: 3.2\n",
      "Number of episodes: 1170 Frame number: 419151 Rewards mean for the last 10 episodes: 2.7\n",
      "Number of episodes: 1180 Frame number: 423440 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 1190 Frame number: 427452 Rewards mean for the last 10 episodes: 3.2\n",
      "Number of episodes: 1200 Frame number: 432226 Rewards mean for the last 10 episodes: 3.4\n",
      "Number of episodes: 1210 Frame number: 436334 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 1220 Frame number: 440446 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 1230 Frame number: 444384 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 1240 Frame number: 449297 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 1250 Frame number: 453596 Rewards mean for the last 10 episodes: 3.2\n",
      "Number of episodes: 1260 Frame number: 457549 Rewards mean for the last 10 episodes: 3.8\n",
      "Number of episodes: 1270 Frame number: 462777 Rewards mean for the last 10 episodes: 3.6\n",
      "Number of episodes: 1280 Frame number: 467345 Rewards mean for the last 10 episodes: 2.2\n",
      "Number of episodes: 1290 Frame number: 471281 Rewards mean for the last 10 episodes: 2.9\n",
      "Number of episodes: 1300 Frame number: 475797 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 1310 Frame number: 479608 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 1320 Frame number: 484757 Rewards mean for the last 10 episodes: 3.4\n",
      "Number of episodes: 1330 Frame number: 489208 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 1340 Frame number: 493411 Rewards mean for the last 10 episodes: 3.8\n",
      "Number of episodes: 1350 Frame number: 497991 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 1360 Frame number: 502839 Rewards mean for the last 10 episodes: 2.9\n",
      "Number of episodes: 1370 Frame number: 507697 Rewards mean for the last 10 episodes: 3.3\n",
      "Number of episodes: 1380 Frame number: 512105 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 1390 Frame number: 516777 Rewards mean for the last 10 episodes: 3.1\n",
      "Number of episodes: 1400 Frame number: 520857 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 1410 Frame number: 525225 Rewards mean for the last 10 episodes: 3.3\n",
      "Number of episodes: 1420 Frame number: 529588 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 1430 Frame number: 533793 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 1440 Frame number: 538309 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 1450 Frame number: 542354 Rewards mean for the last 10 episodes: 3.6\n",
      "Number of episodes: 1460 Frame number: 546563 Rewards mean for the last 10 episodes: 2.0\n",
      "Number of episodes: 1470 Frame number: 550259 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 1480 Frame number: 554977 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 1490 Frame number: 558285 Rewards mean for the last 10 episodes: 1.3\n",
      "Number of episodes: 1500 Frame number: 563118 Rewards mean for the last 10 episodes: 2.9\n",
      "Number of episodes: 1510 Frame number: 568947 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 1520 Frame number: 574238 Rewards mean for the last 10 episodes: 3.1\n",
      "Number of episodes: 1530 Frame number: 578668 Rewards mean for the last 10 episodes: 2.7\n",
      "Number of episodes: 1540 Frame number: 583299 Rewards mean for the last 10 episodes: 2.9\n",
      "Number of episodes: 1550 Frame number: 588421 Rewards mean for the last 10 episodes: 3.3\n",
      "Number of episodes: 1560 Frame number: 593172 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 1570 Frame number: 597660 Rewards mean for the last 10 episodes: 2.8\n",
      "Evaluation score:\n",
      " 0.0\n",
      "600698\n",
      "Number of episodes: 1580 Frame number: 602061 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 1590 Frame number: 606652 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 1600 Frame number: 610333 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 1610 Frame number: 614295 Rewards mean for the last 10 episodes: 1.4\n",
      "Number of episodes: 1620 Frame number: 618364 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 1630 Frame number: 622709 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 1640 Frame number: 627056 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 1650 Frame number: 631537 Rewards mean for the last 10 episodes: 3.7\n",
      "Number of episodes: 1660 Frame number: 634989 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 1670 Frame number: 638645 Rewards mean for the last 10 episodes: 3.2\n",
      "Number of episodes: 1680 Frame number: 642753 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 1690 Frame number: 646625 Rewards mean for the last 10 episodes: 2.1\n",
      "Number of episodes: 1700 Frame number: 651456 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 1710 Frame number: 655425 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 1720 Frame number: 659005 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 1730 Frame number: 663905 Rewards mean for the last 10 episodes: 3.3\n",
      "Number of episodes: 1740 Frame number: 668314 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 1750 Frame number: 672459 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 1760 Frame number: 676917 Rewards mean for the last 10 episodes: 4.3\n",
      "Number of episodes: 1770 Frame number: 681268 Rewards mean for the last 10 episodes: 3.2\n",
      "Number of episodes: 1780 Frame number: 686231 Rewards mean for the last 10 episodes: 3.3\n",
      "Number of episodes: 1790 Frame number: 689937 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 1800 Frame number: 694473 Rewards mean for the last 10 episodes: 2.7\n",
      "Number of episodes: 1810 Frame number: 698805 Rewards mean for the last 10 episodes: 3.2\n",
      "Number of episodes: 1820 Frame number: 702639 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 1830 Frame number: 706117 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 1840 Frame number: 710891 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 1850 Frame number: 715151 Rewards mean for the last 10 episodes: 2.9\n",
      "Number of episodes: 1860 Frame number: 719521 Rewards mean for the last 10 episodes: 2.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes: 1870 Frame number: 723949 Rewards mean for the last 10 episodes: 2.9\n",
      "Number of episodes: 1880 Frame number: 728325 Rewards mean for the last 10 episodes: 2.9\n",
      "Number of episodes: 1890 Frame number: 732224 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 1900 Frame number: 736585 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 1910 Frame number: 740299 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 1920 Frame number: 743802 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 1930 Frame number: 747333 Rewards mean for the last 10 episodes: 2.7\n",
      "Number of episodes: 1940 Frame number: 751441 Rewards mean for the last 10 episodes: 3.9\n",
      "Number of episodes: 1950 Frame number: 756453 Rewards mean for the last 10 episodes: 3.1\n",
      "Number of episodes: 1960 Frame number: 760645 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 1970 Frame number: 764701 Rewards mean for the last 10 episodes: 3.1\n",
      "Number of episodes: 1980 Frame number: 769493 Rewards mean for the last 10 episodes: 3.4\n",
      "Number of episodes: 1990 Frame number: 773880 Rewards mean for the last 10 episodes: 3.2\n",
      "Number of episodes: 2000 Frame number: 778921 Rewards mean for the last 10 episodes: 3.2\n",
      "Number of episodes: 2010 Frame number: 783350 Rewards mean for the last 10 episodes: 3.7\n",
      "Number of episodes: 2020 Frame number: 788221 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 2030 Frame number: 792766 Rewards mean for the last 10 episodes: 3.8\n",
      "Number of episodes: 2040 Frame number: 797489 Rewards mean for the last 10 episodes: 3.7\n",
      "Evaluation score:\n",
      " 0.0\n",
      "800857\n",
      "Number of episodes: 2050 Frame number: 801301 Rewards mean for the last 10 episodes: 1.7\n",
      "Number of episodes: 2060 Frame number: 806526 Rewards mean for the last 10 episodes: 4.3\n",
      "Number of episodes: 2070 Frame number: 810609 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 2080 Frame number: 815092 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 2090 Frame number: 819166 Rewards mean for the last 10 episodes: 4.6\n",
      "Number of episodes: 2100 Frame number: 823637 Rewards mean for the last 10 episodes: 3.1\n",
      "Number of episodes: 2110 Frame number: 827317 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 2120 Frame number: 832165 Rewards mean for the last 10 episodes: 3.1\n",
      "Number of episodes: 2130 Frame number: 836645 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 2140 Frame number: 840577 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 2150 Frame number: 844113 Rewards mean for the last 10 episodes: 1.9\n",
      "Number of episodes: 2160 Frame number: 848095 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 2170 Frame number: 852355 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 2180 Frame number: 856664 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 2190 Frame number: 860829 Rewards mean for the last 10 episodes: 2.1\n",
      "Number of episodes: 2200 Frame number: 865265 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 2210 Frame number: 869157 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 2220 Frame number: 873445 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 2230 Frame number: 877721 Rewards mean for the last 10 episodes: 4.1\n",
      "Number of episodes: 2240 Frame number: 882098 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 2250 Frame number: 885649 Rewards mean for the last 10 episodes: 1.8\n",
      "Number of episodes: 2260 Frame number: 889741 Rewards mean for the last 10 episodes: 3.2\n",
      "Number of episodes: 2270 Frame number: 894603 Rewards mean for the last 10 episodes: 2.7\n",
      "Number of episodes: 2280 Frame number: 899065 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 2290 Frame number: 903253 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 2300 Frame number: 907665 Rewards mean for the last 10 episodes: 3.2\n",
      "Number of episodes: 2310 Frame number: 911785 Rewards mean for the last 10 episodes: 3.3\n",
      "Number of episodes: 2320 Frame number: 915974 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 2330 Frame number: 919873 Rewards mean for the last 10 episodes: 1.9\n",
      "Number of episodes: 2340 Frame number: 923857 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 2350 Frame number: 928098 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 2360 Frame number: 931948 Rewards mean for the last 10 episodes: 2.7\n",
      "Number of episodes: 2370 Frame number: 935738 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 2380 Frame number: 939653 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 2390 Frame number: 943821 Rewards mean for the last 10 episodes: 3.9\n",
      "Number of episodes: 2400 Frame number: 948025 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 2410 Frame number: 951621 Rewards mean for the last 10 episodes: 2.2\n",
      "Number of episodes: 2420 Frame number: 955449 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 2430 Frame number: 959256 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 2440 Frame number: 964057 Rewards mean for the last 10 episodes: 3.3\n",
      "Number of episodes: 2450 Frame number: 968270 Rewards mean for the last 10 episodes: 3.6\n",
      "Number of episodes: 2460 Frame number: 971919 Rewards mean for the last 10 episodes: 1.4\n",
      "Number of episodes: 2470 Frame number: 976055 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 2480 Frame number: 979533 Rewards mean for the last 10 episodes: 2.2\n",
      "Number of episodes: 2490 Frame number: 983421 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 2500 Frame number: 987426 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 2510 Frame number: 991121 Rewards mean for the last 10 episodes: 2.1\n",
      "Number of episodes: 2520 Frame number: 995225 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 2530 Frame number: 999455 Rewards mean for the last 10 episodes: 2.8\n",
      "Evaluation score:\n",
      " 0.0\n",
      "1001037\n",
      "Number of episodes: 2540 Frame number: 1003313 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 2550 Frame number: 1007077 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 2560 Frame number: 1010765 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 2570 Frame number: 1014732 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 2580 Frame number: 1018085 Rewards mean for the last 10 episodes: 2.1\n",
      "Number of episodes: 2590 Frame number: 1022453 Rewards mean for the last 10 episodes: 2.0\n",
      "Number of episodes: 2600 Frame number: 1026435 Rewards mean for the last 10 episodes: 3.4\n",
      "Number of episodes: 2610 Frame number: 1029985 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 2620 Frame number: 1034240 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 2630 Frame number: 1038456 Rewards mean for the last 10 episodes: 2.9\n",
      "Number of episodes: 2640 Frame number: 1041724 Rewards mean for the last 10 episodes: 1.7\n",
      "Number of episodes: 2650 Frame number: 1046461 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 2660 Frame number: 1050389 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 2670 Frame number: 1054624 Rewards mean for the last 10 episodes: 3.1\n",
      "Number of episodes: 2680 Frame number: 1058298 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 2690 Frame number: 1061575 Rewards mean for the last 10 episodes: 1.8\n",
      "Number of episodes: 2700 Frame number: 1065557 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 2710 Frame number: 1069615 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 2720 Frame number: 1073253 Rewards mean for the last 10 episodes: 1.8\n",
      "Number of episodes: 2730 Frame number: 1076304 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 2740 Frame number: 1080333 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 2750 Frame number: 1084381 Rewards mean for the last 10 episodes: 3.3\n",
      "Number of episodes: 2760 Frame number: 1088416 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 2770 Frame number: 1092606 Rewards mean for the last 10 episodes: 3.9\n",
      "Number of episodes: 2780 Frame number: 1096445 Rewards mean for the last 10 episodes: 2.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes: 2790 Frame number: 1099965 Rewards mean for the last 10 episodes: 1.7\n",
      "Number of episodes: 2800 Frame number: 1104204 Rewards mean for the last 10 episodes: 2.0\n",
      "Number of episodes: 2810 Frame number: 1108061 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 2820 Frame number: 1111849 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 2830 Frame number: 1115673 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 2840 Frame number: 1119484 Rewards mean for the last 10 episodes: 3.1\n",
      "Number of episodes: 2850 Frame number: 1122938 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 2860 Frame number: 1126309 Rewards mean for the last 10 episodes: 1.6\n",
      "Number of episodes: 2870 Frame number: 1129997 Rewards mean for the last 10 episodes: 2.2\n",
      "Number of episodes: 2880 Frame number: 1133805 Rewards mean for the last 10 episodes: 2.9\n",
      "Number of episodes: 2890 Frame number: 1137785 Rewards mean for the last 10 episodes: 3.2\n",
      "Number of episodes: 2900 Frame number: 1141248 Rewards mean for the last 10 episodes: 2.7\n",
      "Number of episodes: 2910 Frame number: 1144755 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 2920 Frame number: 1148520 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 2930 Frame number: 1151885 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 2940 Frame number: 1156239 Rewards mean for the last 10 episodes: 3.3\n",
      "Number of episodes: 2950 Frame number: 1160077 Rewards mean for the last 10 episodes: 2.7\n",
      "Number of episodes: 2960 Frame number: 1163934 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 2970 Frame number: 1167427 Rewards mean for the last 10 episodes: 1.9\n",
      "Number of episodes: 2980 Frame number: 1170909 Rewards mean for the last 10 episodes: 1.9\n",
      "Number of episodes: 2990 Frame number: 1174760 Rewards mean for the last 10 episodes: 3.5\n",
      "Number of episodes: 3000 Frame number: 1178563 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 3010 Frame number: 1182411 Rewards mean for the last 10 episodes: 3.2\n",
      "Number of episodes: 3020 Frame number: 1186157 Rewards mean for the last 10 episodes: 2.7\n",
      "Number of episodes: 3030 Frame number: 1190321 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 3040 Frame number: 1194018 Rewards mean for the last 10 episodes: 3.6\n",
      "Number of episodes: 3050 Frame number: 1198345 Rewards mean for the last 10 episodes: 2.8\n",
      "Evaluation score:\n",
      " 0.0\n",
      "1201277\n",
      "Number of episodes: 3060 Frame number: 1202196 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 3070 Frame number: 1206233 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 3080 Frame number: 1210292 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 3090 Frame number: 1213861 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 3100 Frame number: 1218109 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 3110 Frame number: 1221917 Rewards mean for the last 10 episodes: 2.2\n",
      "Number of episodes: 3120 Frame number: 1225713 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 3130 Frame number: 1229722 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 3140 Frame number: 1233405 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 3150 Frame number: 1237129 Rewards mean for the last 10 episodes: 1.4\n",
      "Number of episodes: 3160 Frame number: 1240485 Rewards mean for the last 10 episodes: 3.1\n",
      "Number of episodes: 3170 Frame number: 1244505 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 3180 Frame number: 1247877 Rewards mean for the last 10 episodes: 3.1\n",
      "Number of episodes: 3190 Frame number: 1252137 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 3200 Frame number: 1255625 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 3210 Frame number: 1259422 Rewards mean for the last 10 episodes: 1.9\n",
      "Number of episodes: 3220 Frame number: 1262744 Rewards mean for the last 10 episodes: 2.0\n",
      "Number of episodes: 3230 Frame number: 1267232 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 3240 Frame number: 1272168 Rewards mean for the last 10 episodes: 4.0\n",
      "Number of episodes: 3250 Frame number: 1276545 Rewards mean for the last 10 episodes: 3.7\n",
      "Number of episodes: 3260 Frame number: 1280437 Rewards mean for the last 10 episodes: 2.7\n",
      "Number of episodes: 3270 Frame number: 1284032 Rewards mean for the last 10 episodes: 2.1\n",
      "Number of episodes: 3280 Frame number: 1287269 Rewards mean for the last 10 episodes: 2.0\n",
      "Number of episodes: 3290 Frame number: 1291341 Rewards mean for the last 10 episodes: 3.1\n",
      "Number of episodes: 3300 Frame number: 1294903 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 3310 Frame number: 1298285 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 3320 Frame number: 1301748 Rewards mean for the last 10 episodes: 2.7\n",
      "Number of episodes: 3330 Frame number: 1304971 Rewards mean for the last 10 episodes: 1.9\n",
      "Number of episodes: 3340 Frame number: 1308854 Rewards mean for the last 10 episodes: 3.4\n",
      "Number of episodes: 3350 Frame number: 1312941 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 3360 Frame number: 1316877 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 3370 Frame number: 1321477 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 3380 Frame number: 1325366 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 3390 Frame number: 1328547 Rewards mean for the last 10 episodes: 2.7\n",
      "Number of episodes: 3400 Frame number: 1332807 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 3410 Frame number: 1336109 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 3420 Frame number: 1339641 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 3430 Frame number: 1343183 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 3440 Frame number: 1346940 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 3450 Frame number: 1350341 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 3460 Frame number: 1354544 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 3470 Frame number: 1358214 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 3480 Frame number: 1361825 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 3490 Frame number: 1365427 Rewards mean for the last 10 episodes: 3.4\n",
      "Number of episodes: 3500 Frame number: 1368827 Rewards mean for the last 10 episodes: 2.1\n",
      "Number of episodes: 3510 Frame number: 1372352 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 3520 Frame number: 1375421 Rewards mean for the last 10 episodes: 1.7\n",
      "Number of episodes: 3530 Frame number: 1379237 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 3540 Frame number: 1383179 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 3550 Frame number: 1386945 Rewards mean for the last 10 episodes: 2.1\n",
      "Number of episodes: 3560 Frame number: 1390497 Rewards mean for the last 10 episodes: 2.0\n",
      "Number of episodes: 3570 Frame number: 1394353 Rewards mean for the last 10 episodes: 2.9\n",
      "Number of episodes: 3580 Frame number: 1397431 Rewards mean for the last 10 episodes: 2.1\n",
      "Number of episodes: 3590 Frame number: 1400635 Rewards mean for the last 10 episodes: 1.9\n",
      "Evaluation score:\n",
      " 11.0\n",
      "1401321\n",
      "Number of episodes: 3600 Frame number: 1404173 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 3610 Frame number: 1407601 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 3620 Frame number: 1411197 Rewards mean for the last 10 episodes: 3.4\n",
      "Number of episodes: 3630 Frame number: 1414691 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 3640 Frame number: 1418605 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 3650 Frame number: 1422211 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 3660 Frame number: 1425673 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 3670 Frame number: 1429748 Rewards mean for the last 10 episodes: 3.4\n",
      "Number of episodes: 3680 Frame number: 1433583 Rewards mean for the last 10 episodes: 3.1\n",
      "Number of episodes: 3690 Frame number: 1436929 Rewards mean for the last 10 episodes: 2.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes: 3700 Frame number: 1441411 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 3710 Frame number: 1444871 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 3720 Frame number: 1448533 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 3730 Frame number: 1452583 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 3740 Frame number: 1456293 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 3750 Frame number: 1459705 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 3760 Frame number: 1463365 Rewards mean for the last 10 episodes: 4.2\n",
      "Number of episodes: 3770 Frame number: 1466753 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 3780 Frame number: 1470504 Rewards mean for the last 10 episodes: 2.7\n",
      "Number of episodes: 3790 Frame number: 1474033 Rewards mean for the last 10 episodes: 1.5\n",
      "Number of episodes: 3800 Frame number: 1477307 Rewards mean for the last 10 episodes: 2.4\n",
      "Number of episodes: 3810 Frame number: 1481327 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 3820 Frame number: 1485031 Rewards mean for the last 10 episodes: 2.6\n",
      "Number of episodes: 3830 Frame number: 1488569 Rewards mean for the last 10 episodes: 2.2\n",
      "Number of episodes: 3840 Frame number: 1491601 Rewards mean for the last 10 episodes: 1.8\n",
      "Number of episodes: 3850 Frame number: 1495097 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 3860 Frame number: 1498823 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 3870 Frame number: 1502253 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 3880 Frame number: 1506151 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 3890 Frame number: 1509457 Rewards mean for the last 10 episodes: 1.9\n",
      "Number of episodes: 3900 Frame number: 1512751 Rewards mean for the last 10 episodes: 2.5\n",
      "Number of episodes: 3910 Frame number: 1516301 Rewards mean for the last 10 episodes: 2.7\n",
      "Number of episodes: 3920 Frame number: 1519840 Rewards mean for the last 10 episodes: 3.3\n",
      "Number of episodes: 3930 Frame number: 1523377 Rewards mean for the last 10 episodes: 1.6\n",
      "Number of episodes: 3940 Frame number: 1527251 Rewards mean for the last 10 episodes: 3.1\n",
      "Number of episodes: 3950 Frame number: 1531024 Rewards mean for the last 10 episodes: 3.3\n",
      "Number of episodes: 3960 Frame number: 1535169 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 3970 Frame number: 1538737 Rewards mean for the last 10 episodes: 3.0\n",
      "Number of episodes: 3980 Frame number: 1542213 Rewards mean for the last 10 episodes: 1.9\n",
      "Number of episodes: 3990 Frame number: 1546224 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 4000 Frame number: 1550063 Rewards mean for the last 10 episodes: 2.3\n",
      "Number of episodes: 4010 Frame number: 1553369 Rewards mean for the last 10 episodes: 2.7\n",
      "Number of episodes: 4020 Frame number: 1557140 Rewards mean for the last 10 episodes: 2.7\n",
      "Number of episodes: 4030 Frame number: 1560869 Rewards mean for the last 10 episodes: 3.2\n",
      "Number of episodes: 4040 Frame number: 1564427 Rewards mean for the last 10 episodes: 2.8\n",
      "Number of episodes: 4050 Frame number: 1568537 Rewards mean for the last 10 episodes: 2.9\n",
      "Number of episodes: 4060 Frame number: 1571877 Rewards mean for the last 10 episodes: 2.1\n",
      "Number of episodes: 4070 Frame number: 1575501 Rewards mean for the last 10 episodes: 1.9\n",
      "Number of episodes: 4080 Frame number: 1579444 Rewards mean for the last 10 episodes: 3.2\n",
      "Number of episodes: 4090 Frame number: 1582920 Rewards mean for the last 10 episodes: 2.6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-831e35bdf792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-ebd0dc0d8ba8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mframe_number\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mUPDATE_FREQ\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mframe_number\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mREPLAY_MEMORY_START_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_replay_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAIN_DDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTARGET_DDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDISCOUNT_FACTOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                         \u001b[0mloss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f80c3a35da45>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(session, replay_memory, main_ddqn, target_ddqn, batch_size, gamma)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal_flags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mbest_new_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_ddqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmain_ddqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mq_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_ddqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtarget_ddqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/final_project/final_project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/final_project/final_project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/final_project/final_project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/final_project/final_project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/final_project/final_project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/final_project/final_project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
