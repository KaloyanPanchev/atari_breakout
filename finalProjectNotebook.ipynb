{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import retro\n",
    "import gym\n",
    "import os\n",
    "\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of frame:  Box(210, 160, 3)\n",
      "number of actions:  4\n",
      "actions:  ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "ACTION_SIZE = env.env.action_space.n\n",
    "possible_actions = env.unwrapped.get_action_meanings()\n",
    "\n",
    "print(\"size of frame: \", env.observation_space)\n",
    "print(\"number of actions: \", ACTION_SIZE)\n",
    "print(\"actions: \", possible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 4\n",
      "0.0 False 4\n",
      "0.0 False 4\n",
      "0.0 False 4\n",
      "0.0 False 4\n",
      "0.0 False 4\n"
     ]
    }
   ],
   "source": [
    "#try a few actions and games:\n",
    "frame = env.reset()\n",
    "for i in range(30):\n",
    "    new_frame, reward, terminal, info = env.step(1)\n",
    "    print(reward, terminal, info['ale.lives'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameProcessor(object):\n",
    "    def __init__(self, height=84, width = 84):\n",
    "        self.height = height\n",
    "        self.widht = width\n",
    "        self.frame = tf.placeholder(shape=[210,160,3], dtype=tf.uint8)\n",
    "        \n",
    "        self.gray_scaled = tf.image.rgb_to_grayscale(frame)\n",
    "        self.cropped_frame = tf.image.crop_to_bounding_box(self.gray_scaled,34, 0, 160, 160)\n",
    "        self.processed = tf.image.resize_images(self.cropped_frame, [84, 84] , method = tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    def __call__(self, session, frame):\n",
    "        return session.run(self.processed, feed_dict={self.frame:frame})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gif(frame_number, frames_for_stack, reward, path):\n",
    "    for idx, frame_idx in enumerate(frames_for_stack):\n",
    "        frames_for_stack[idx] = resize(frame_idx, (420, 320, 3), preserve_range=True, order=0).astype(np.uint8)\n",
    "    \n",
    "    imageio.mimsave(f'{path}{\"Breakout_frame_{0}_reward_{1}.gif\".format(frame_number,reward)}', frames_for_stack, duration =1/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(object):\n",
    "    def __init__(self, number_actions, hidden=512, learning_rate=0.0000625, height=84, width=84, history_length=4):\n",
    "        self.number_actions = number_actions\n",
    "        self.hidden = hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.history_length = history_length\n",
    "        \n",
    "        self.input = tf.placeholder(shape=[None, self.height, self.width, self.history_length], dtype=tf.float32)\n",
    "        self.input = self.input/255\n",
    "        \n",
    "        #CONV LAYERS:\n",
    "        \n",
    "        #CONV 1:\n",
    "        self.conv1 = tf.layers.conv2d(inputs=self.input, filters=32, kernel_size=[8, 8], strides=4,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                     padding=\"valid\", activation=tf.nn.elu, use_bias=False, name=\"conv1\")\n",
    "        \n",
    "        #CONV 2:\n",
    "        self.conv2 = tf.layers.conv2d(inputs=self.conv1, filters=64, kernel_size=[4, 4], strides=2,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                     padding=\"valid\", activation=tf.nn.elu, use_bias=False, name=\"conv2\")\n",
    "        \n",
    "        #CONV 3:\n",
    "        self.conv3 = tf.layers.conv2d(inputs=self.conv2, filters=64, kernel_size=[3, 3], strides=1,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                     padding=\"valid\", activation=tf.nn.elu, use_bias=False, name=\"conv3\")\n",
    "        \n",
    "        self.flatten = tf.layers.flatten(self.conv3)\n",
    "        \n",
    "        \n",
    "        #Calculate V(s)\n",
    "        self.value_fc = tf.layers.dense(inputs = self.flatten, units=hidden, activation=tf.nn.elu,\n",
    "                                        kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"value_fc\")\n",
    "        \n",
    "        self.value = tf.layers.dense(inputs = self.value_fc, units=1, activation=None,\n",
    "                                        kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"value\")\n",
    "        \n",
    "        #Calculate A(s,a)\n",
    "        self.advantage_fc = tf.layers.dense(inputs = self.flatten, units=hidden, activation=tf.nn.elu,\n",
    "                                        kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"advantage_fc\")\n",
    "        \n",
    "        self.advantage = tf.layers.dense(inputs = self.advantage_fc, units=self.number_actions, activation=None,\n",
    "                                        kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"advantages\")\n",
    "\n",
    "        \n",
    "        #Combine the two:\n",
    "        self.q_values = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "        self.best_action = tf.argmax(self.q_values, 1)\n",
    "        \n",
    "        #target q:\n",
    "        self.target_q = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        \n",
    "        #action we took:\n",
    "        self.action = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        \n",
    "        #Q value of the action above:\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.q_values, tf.one_hot(self.action, self.number_actions, dtype=tf.float32))\n",
    "                               , axis=1)\n",
    "        \n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.losses.huber_loss(labels=self.target_q, predictions= self.Q))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.update = self.optimizer.minimize(self.loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(session, DDQN, explore_start, explore_stop, decay_rate, decay_step, state, action_size, evaluation = False):\n",
    "    exp_tradeoff = np.random.rand()\n",
    "    \n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if evaluation:\n",
    "        explore_probability = 0.0\n",
    "    \n",
    "    if(explore_probability > exp_tradeoff):\n",
    "        #random action(exploration)\n",
    "        action = np.random.randint(0, action_size)\n",
    "    else:\n",
    "        action = session.run(DDQN.best_action, feed_dict={DDQN.input:[state]})[0]\n",
    "        \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, size=100000, height=84, width= 84, history_length=4, batch_size=32):\n",
    "        self.size = size\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.history_length = history_length\n",
    "        self.batch_size = batch_size\n",
    "        self.count = 0\n",
    "        self.current= 0\n",
    "        \n",
    "        #where the transitions will be stored:\n",
    "        self.actions = np.empty(self.size, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size, dtype=np.int32)\n",
    "        self.frames = np.empty((self.size, self.height, self.width), dtype=np.uint8)\n",
    "        self.terminal_flags = np.empty(self.size, dtype=np.int32)\n",
    "        \n",
    "        #memory for the minibatch:\n",
    "        self.states = np.empty((self.batch_size, self.history_length, self.height, self.width),\n",
    "                               dtype=np.uint8)\n",
    "        self.new_states = np.empty((self.batch_size, self.history_length, self.height, self.width),\n",
    "                               dtype=np.uint8)\n",
    "        self.indices = np.empty(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "    def add_experience(self, action, frame, reward, terminal):\n",
    "        if frame.shape != (self.height, self.width):\n",
    "            raise ValueError(\"Dimensions of frame do not match 84x84\")\n",
    "            \n",
    "        #add the experience:\n",
    "        self.actions[self.current] = action\n",
    "        self.frames[self.current, ...] = frame\n",
    "        self.rewards[self.current] = reward\n",
    "        self.terminal_flags[self.current] = terminal\n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current + 1) % self.size # if we reach the limit we start overriding the first ones\n",
    "        \n",
    "    def _get_state(self, index):\n",
    "        if self.count is 0:\n",
    "            raise ValueError(\"the memory is empty\")\n",
    "        if index < 3:\n",
    "            raise ValueError(\"index must be at least 3\")\n",
    "        return self.frames[index-self.history_length+1:index+1, ...] #get the 4 frames that represent this state\n",
    "            \n",
    "    def _get_valid_indices(self):\n",
    "        for i in range(self.batch_size):\n",
    "            while True:\n",
    "                index = random.randint(self.history_length, self.count - 1)\n",
    "                if index < self.history_length: # index cannot be smalled than 4\n",
    "                    continue\n",
    "                if index >= self.current and index - self.history_length <= self.current: # there should be atleast 4 frames to get after the state\n",
    "                    continue\n",
    "                if self.terminal_flags[index - self.history_length:index].any(): #if there is a terminal flag active, that means that in those four frame the agent died => we do not want to take them as a state\n",
    "                    continue\n",
    "                break\n",
    "            self.indices[i] = index\n",
    "        \n",
    "    def get_minibatch(self):\n",
    "            \n",
    "        if self.count < self.history_length:\n",
    "            raise ValueError(\"not enough memories to get a minibatch\")\n",
    "                \n",
    "        self._get_valid_indices()\n",
    "            \n",
    "        for i, idx in enumerate(self.indices):\n",
    "            self.states[i] = self._get_state(idx - 1)\n",
    "            self.new_states[i] = self._get_state(idx)\n",
    "                \n",
    "        return np.transpose(self.states, axes=(0,2,3,1)), self.actions[self.indices], self.rewards[self.indices], np.transpose(self.new_states, axes=(0,2,3,1)), self.terminal_flags[self.indices]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(session, replay_memory, main_ddqn, target_ddqn, batch_size, gamma):\n",
    "    states, actions, rewards, new_states, terminal_flags = replay_memory.get_minibatch()\n",
    "    \n",
    "    best_new_actions = session.run(main_ddqn.best_action, feed_dict={main_ddqn.input:new_states})\n",
    "    \n",
    "    q_vals = session.run(target_ddqn.q_values, feed_dict={target_ddqn.input:new_states})\n",
    "    double_q = q_vals[range(batch_size), best_new_actions]\n",
    "    \n",
    "    target_q = rewards + (gamma*double_q*(1 - terminal_flags))\n",
    "    \n",
    "    loss, _= session.run([main_ddqn.loss, main_ddqn.update]\n",
    "                         , feed_dict={main_ddqn.input:states, main_ddqn.target_q:target_q, main_ddqn.action:actions})\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetNetworkUpdater(object):\n",
    "    def __init__(self, main_ddqn_vars, target_ddqn_vars):\n",
    "        self.main_ddqn_vars = main_ddqn_vars\n",
    "        self.target_ddqn_vars = target_ddqn_vars\n",
    "        \n",
    "    def _update_target_vars(self):\n",
    "        update_ops = []\n",
    "        for i, var in enumerate(self.main_ddqn_vars):\n",
    "            copy_op = self.target_ddqn_vars[i].assign(var.value())\n",
    "            update_ops.append(copy_op)\n",
    "        return update_ops\n",
    "    \n",
    "    def __call__(self, sess):\n",
    "        update_ops = self._update_target_vars()\n",
    "        for copy_op in update_ops:\n",
    "            sess.run(copy_op)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreakoutGame(object):\n",
    "    \n",
    "    def __init__(self, env_name=\"BreakoutDeterministic-v4\", no_op_steps=10, history_length=4):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.process_frame = FrameProcessor()\n",
    "        self.state = None\n",
    "        self.last_lives = 0\n",
    "        self.no_op_steps = no_op_steps\n",
    "        self.history_length = history_length\n",
    "        \n",
    "    def reset(self, sess, evaluation=False):\n",
    "        frame = self.env.reset()\n",
    "        self.last_lives = 0\n",
    "        terminal_life_lost = True\n",
    "        \n",
    "        if evaluation:\n",
    "            for _ in range(random.randint(1, self.no_op_steps)):\n",
    "                frame, _, _, _ = self.env.step(1) #fire the ball\n",
    "                \n",
    "        processed_frame = self.process_frame(sess, frame)\n",
    "        self.state = np.repeat(processed_frame, self.history_length, axis=2)\n",
    "        \n",
    "        return terminal_life_lost\n",
    "    \n",
    "    def step(self, sess, action):\n",
    "        \n",
    "        new_frame, reward, terminal, info = self.env.step(action)\n",
    "        \n",
    "        if info['ale.lives'] < self.last_lives:\n",
    "            terminal_life_lost = True\n",
    "        else:\n",
    "            terminal_life_lost = terminal\n",
    "        self.last_lives = info['ale.lives']\n",
    "        \n",
    "        processed_new_frame = self.process_frame(sess, new_frame)\n",
    "        new_state = np.append(self.state[:, :, 1:], processed_new_frame, axis=2)\n",
    "        self.state = new_state\n",
    "        \n",
    "        return processed_new_frame, reward, terminal, terminal_life_lost, new_frame\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip the rewards to be -1, 0, 1. Proved to be benefitial for Breakout\n",
    "def clip_reward(reward):\n",
    "    if reward > 0:\n",
    "        return 1\n",
    "    elif reward == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the hyper-params:\n",
    "\n",
    "MAX_EPISODE_LENGTH = 180\n",
    "EVAL_FREQUENCY = 2000\n",
    "EVAL_STEPS = 100\n",
    "TARGET_NETWORK_UPDATE_FREQ = 1000\n",
    "\n",
    "DISCOUNT_FACTOR = 0.95 # Gamma in the Q learning equation\n",
    "REPLAY_MEMORY_START_SIZE = 500 # random actions at the begining\n",
    "\n",
    "MAX_FRAMES = 70000\n",
    "MEMORY_SIZE = 1000000\n",
    "NO_OP_STEPS = 10\n",
    "\n",
    "UPDATE_FREQ = 4\n",
    "HIDDEN = 512\n",
    "\n",
    "LEARNING_RATE = 0.0000625\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "EXPLORE_START = 1.0\n",
    "EXPLORE_STOP = 0.01\n",
    "DECAY_RATE = 0.00005\n",
    "\n",
    "#IMPORTANT FOR TRAINING OR TESTING:\n",
    "PATH = \"output/\"\n",
    "INFO = \"info\"\n",
    "RUNID = \"run1\"\n",
    "TRAIN = True\n",
    "\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "os.makedirs(os.path.join(INFO, RUNID), exist_ok=True)\n",
    "SUMM_WRITER = tf.summary.FileWriter(os.path.join(INFO, RUNID))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kaloyan/deeplearning/final_project/final_project/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-6-7c1f8e3b3f40>:18: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-7c1f8e3b3f40>:30: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-7c1f8e3b3f40>:35: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/kaloyan/deeplearning/final_project/final_project/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:448: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/kaloyan/deeplearning/final_project/final_project/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "atari = BreakoutGame()\n",
    "\n",
    "#create main DQN:\n",
    "with tf.variable_scope('mainDDQN'):\n",
    "    MAIN_DDQN = DDQN(atari.env.action_space.n, HIDDEN, LEARNING_RATE)\n",
    "\n",
    "MAIN_DDQN_VARS = tf.trainable_variables(scope='mainDDQN')\n",
    "\n",
    "\n",
    "#create target DQN:\n",
    "with tf.variable_scope('targetDDQN'):\n",
    "    TARGET_DDQN = DDQN(atari.env.action_space.n, HIDDEN)\n",
    "\n",
    "TARGET_DDQN_VARS = tf.trainable_variables(scope='targetDDQN')\n",
    "\n",
    "\n",
    "init= tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS_IDS = [\"conv1\", \"conv2\", \"conv3\", \"value_fc\", \"value\", \"advantage_fc\", \"advantages\"]\n",
    "\n",
    "with tf.name_scope(\"Performace\"):\n",
    "    LOSS_PH = tf.placeholder(tf.float32, shape=None, name=\"loss_summary\")\n",
    "    LOSS_SUMMARY = tf.summary.scalar(\"loss\", LOSS_PH)\n",
    "    REWARD_PH = tf.placeholder(tf.float32, shape=None, name=\"reward_summary\")\n",
    "    REWARD_SUMMARY = tf.summary.scalar(\"reward\", REWARD_PH)\n",
    "    EVAL_SCORE_PH = tf.placeholder(tf.float32, shape=None, name=\"eval_summary\")\n",
    "    EVAL_SCORE_SUMMARY = tf.summary.scalar(\"eval_score\", EVAL_SCORE_PH)\n",
    "    \n",
    "PERFORMANCE_SUMMARIES = tf.summary.merge([LOSS_SUMMARY, REWARD_SUMMARY])\n",
    "\n",
    "with tf.name_scope(\"Parameters\"):\n",
    "    ALL_PARAM_SUMMARIES = []\n",
    "    for i, Id in enumerate(LAYERS_IDS):\n",
    "        with tf.name_scope(\"mainDDQN/\"):\n",
    "            MAIN_DDQN_KERNEL = tf.summary.histogram(Id, tf.reshape(MAIN_DDQN_VARS[i], shape=[-1]))\n",
    "        ALL_PARAM_SUMMARIES.extend([MAIN_DDQN_KERNEL])\n",
    "PARAM_SUMMARIES = tf.summary.merge(ALL_PARAM_SUMMARIES)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    my_replay_memory = ReplayMemory(size=MEMORY_SIZE, batch_size=BATCH_SIZE)\n",
    "    update_networks = TargetNetworkUpdater(MAIN_DDQN_VARS, TARGET_DDQN_VARS)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        frame_number = 0\n",
    "        rewards = []\n",
    "        loss_list = []\n",
    "        \n",
    "        decay_step = 0\n",
    "        tau = 0\n",
    "        \n",
    "        while frame_number < MAX_FRAMES:\n",
    "            print(frame_number)\n",
    "        #training:\n",
    "            epoch_frame = 0\n",
    "            while epoch_frame < EVAL_FREQUENCY:\n",
    "                terminal_life_lost = atari.reset(sess)\n",
    "                episode_reward_sum = 0\n",
    "                for _ in range(MAX_EPISODE_LENGTH):\n",
    "                    \n",
    "                    #take the action:\n",
    "                    action, explore_prob = predict_action(sess, MAIN_DDQN, EXPLORE_START, EXPLORE_STOP,\n",
    "                                                          DECAY_RATE, decay_step, atari.state, action_size=ACTION_SIZE)\n",
    "                    decay_step += 1\n",
    "                    \n",
    "                    processed_new_frame, reward, terminal, terminal_life_lost, _ = atari.step(sess, action)\n",
    "                    \n",
    "                    frame_number += 1\n",
    "                    epoch_frame += 1\n",
    "                    episode_reward_sum += reward\n",
    "                    \n",
    "                    clipped_reward = clip_reward(reward)\n",
    "                    \n",
    "                    my_replay_memory.add_experience(action=action, frame=processed_new_frame[:, :, 0], reward=clipped_reward,\n",
    "                                                   terminal=terminal_life_lost)\n",
    "                    \n",
    "                    if frame_number % UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        loss = learn(sess, my_replay_memory, MAIN_DDQN, TARGET_DDQN, BATCH_SIZE, gamma = DISCOUNT_FACTOR)\n",
    "                        loss_list.append(loss)\n",
    "                    \n",
    "                    if frame_number % TARGET_NETWORK_UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        update_networks(sess)\n",
    "                        \n",
    "                    if terminal:\n",
    "                        terminal = False\n",
    "                        break\n",
    "                \n",
    "                rewards.append(episode_reward_sum)\n",
    "                \n",
    "                if len(rewards) % 10 == 0:\n",
    "                    \n",
    "                    if frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        summ = sess.run(PERFORMANCE_SUMMARIES, feed_dict={LOSS_PH:np.mean(loss_list), REWARD_PH:np.mean(rewards[-100:])})\n",
    "                        \n",
    "                        SUMM_WRITER.add_summary(summ, frame_number)\n",
    "                        loss_list = []\n",
    "                        \n",
    "                    summ_param = sess.run(PARAM_SUMMARIES)\n",
    "                    SUMM_WRITER.add_summary(summ_param, frame_number)\n",
    "                    \n",
    "                    print(\"Number of rewards: {}\".format(len(rewards)), \"Frame number: {}\".format(frame_number),\n",
    "                          \"Rewards mean: {}\".format(np.mean(rewards[-100:])))\n",
    "                    with open(\"rewards.dat\", \"a\") as reward_file:\n",
    "                        print(len(rewards), frame_number, np.mean(rewards[-100:]), file=reward_file)\n",
    "            \n",
    "            #evaluation:\n",
    "            \n",
    "            terminal = True\n",
    "            gif = False\n",
    "            frames_for_gif = []\n",
    "            eval_rewards = []\n",
    "            evaluate_frame_number = 0\n",
    "            \n",
    "            for _ in range(EVAL_STEPS):\n",
    "                if terminal:\n",
    "                    terminal_life_lost = atari.reset(sess, evaluation=True)\n",
    "                    episode_reward_sum = 0\n",
    "                    terminal = False\n",
    "                    \n",
    "                if terminal_life_lost:\n",
    "                    action = 1 \n",
    "                else: \n",
    "                    action, _ = predict_action(sess, MAIN_DDQN, EXPLORE_START, EXPLORE_STOP,\n",
    "                                                                         DECAY_RATE, decay_step, atari.state, ACTION_SIZE, evaluation=True)\n",
    "                \n",
    "                processed_new_frame, reward, terminal, terminal_life_lost, _ = atari.step(sess, action)\n",
    "                evaluate_frame_number += 1\n",
    "                episode_reward_sum += reward\n",
    "                \n",
    "                if gif:\n",
    "                    frames_for_gif.append(new_frame)\n",
    "                if terminal:\n",
    "                    eval_rewards.append(episode_reward_sum)\n",
    "                    gif = False # save only the first game of evaluation\n",
    "                \n",
    "            print(\"Evaluation score:\\n\", np.mean(eval_rewards))\n",
    "            try:\n",
    "                generate_gif(frame_number, frames_for_gif, eval_rewards[0], PATH)\n",
    "            except IndexError:\n",
    "                print(\"the eval game has not finished\")\n",
    "            \n",
    "            #save network params:\n",
    "            #saver.save(sess, PATH+\"/my_model\", global_step=frame_number)\n",
    "            frames_for_gif = []\n",
    "            \n",
    "            summ = sess.run(EVAL_SCORE_SUMMARY, feed_dict={EVAL_SCORE_PH:np.mean(eval_rewards)})\n",
    "            SUMM_WRITER.add_summary(summ, frame_number)\n",
    "            \n",
    "            with open(\"rewardsEval.dat\", \"a\") as eval_reward_file:\n",
    "                 print(frame_number, np.mean(eval_rewards), file=eval_reward_file)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Number of rewards: 10 Frame number: 1638 Rewards mean: 1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaloyan/deeplearning/final_project/final_project/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/kaloyan/deeplearning/final_project/final_project/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "2135\n",
      "Number of rewards: 20 Frame number: 3308 Rewards mean: 1.15\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "4293\n",
      "Number of rewards: 30 Frame number: 4998 Rewards mean: 1.2\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "6403\n",
      "Number of rewards: 40 Frame number: 6566 Rewards mean: 1.075\n",
      "Number of rewards: 50 Frame number: 8216 Rewards mean: 1.0\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "8529\n",
      "Number of rewards: 60 Frame number: 9782 Rewards mean: 0.9666666666666667\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "10591\n",
      "Number of rewards: 70 Frame number: 11361 Rewards mean: 0.9428571428571428\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "12675\n",
      "Number of rewards: 80 Frame number: 13024 Rewards mean: 1.0\n",
      "Number of rewards: 90 Frame number: 14689 Rewards mean: 0.9777777777777777\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "14689\n",
      "Number of rewards: 100 Frame number: 16325 Rewards mean: 0.94\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "16858\n",
      "Number of rewards: 110 Frame number: 18106 Rewards mean: 0.97\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "18953\n",
      "Number of rewards: 120 Frame number: 19781 Rewards mean: 0.97\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "20963\n",
      "Number of rewards: 130 Frame number: 21503 Rewards mean: 0.98\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "23072\n",
      "Number of rewards: 140 Frame number: 23252 Rewards mean: 1.07\n",
      "Number of rewards: 150 Frame number: 24941 Rewards mean: 1.13\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "25081\n",
      "Number of rewards: 160 Frame number: 26657 Rewards mean: 1.15\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "27197\n",
      "Number of rewards: 170 Frame number: 28457 Rewards mean: 1.24\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "29357\n",
      "Number of rewards: 180 Frame number: 30238 Rewards mean: 1.22\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "31453\n",
      "Number of rewards: 190 Frame number: 31964 Rewards mean: 1.24\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "33548\n",
      "Number of rewards: 200 Frame number: 33728 Rewards mean: 1.34\n",
      "Number of rewards: 210 Frame number: 35525 Rewards mean: 1.34\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "35705\n",
      "Number of rewards: 220 Frame number: 37322 Rewards mean: 1.36\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "37862\n",
      "Number of rewards: 230 Frame number: 39104 Rewards mean: 1.34\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "40004\n",
      "Number of rewards: 240 Frame number: 40893 Rewards mean: 1.33\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "42153\n",
      "Number of rewards: 250 Frame number: 42693 Rewards mean: 1.3\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "44313\n",
      "Number of rewards: 260 Frame number: 44493 Rewards mean: 1.3\n",
      "Number of rewards: 270 Frame number: 46293 Rewards mean: 1.24\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "46473\n",
      "Number of rewards: 280 Frame number: 48093 Rewards mean: 1.23\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "48633\n",
      "Number of rewards: 290 Frame number: 49889 Rewards mean: 1.22\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "50789\n",
      "Number of rewards: 300 Frame number: 51689 Rewards mean: 1.18\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "52949\n",
      "Number of rewards: 310 Frame number: 53489 Rewards mean: 1.15\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "55109\n",
      "Number of rewards: 320 Frame number: 55289 Rewards mean: 1.11\n",
      "Number of rewards: 330 Frame number: 57077 Rewards mean: 1.11\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "57257\n",
      "Number of rewards: 340 Frame number: 58877 Rewards mean: 1.1\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "59417\n",
      "Number of rewards: 350 Frame number: 60677 Rewards mean: 1.13\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "61577\n",
      "Number of rewards: 360 Frame number: 62477 Rewards mean: 1.15\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "63737\n",
      "Number of rewards: 370 Frame number: 64277 Rewards mean: 1.12\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "65897\n",
      "Number of rewards: 380 Frame number: 66077 Rewards mean: 1.13\n",
      "Number of rewards: 390 Frame number: 67877 Rewards mean: 1.11\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n",
      "68057\n",
      "Number of rewards: 400 Frame number: 69677 Rewards mean: 1.03\n",
      "Evaluation score:\n",
      " nan\n",
      "the eval game has not finished\n"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
