{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "import imageio\n",
    "from skimage.transform import resize\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove deprication msgs for .layers\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = True\n",
    "ENV_NAME = 'BreakoutDeterministric-v4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of frame:  Box(210, 160, 3)\n",
      "number of actions:  4\n",
      "actions:  ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "ACTION_SIZE = env.env.action_space.n\n",
    "possible_actions = env.unwrapped.get_action_meanings()\n",
    "\n",
    "print(\"size of frame: \", env.observation_space)\n",
    "print(\"number of actions: \", ACTION_SIZE)\n",
    "print(\"actions: \", possible_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameProcessor(object):\n",
    "    def __init__(self, frame_height=84, frame_width=84):\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.frame = tf.placeholder(shape=[210,160,3], dtype=tf.uint8)\n",
    "        self.processed = tf.image.rgb_to_grayscale(self.frame)\n",
    "        self.processed = tf.image.crop_to_bounding_box(self.processed,34, 0, 160, 160)\n",
    "        self.processed = tf.image.resize_images(self.processed, [self.frame_height, frame_width],\n",
    "                                                method = tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    def __call__(self, session, frame):\n",
    "        return session.run(self.processed, feed_dict={self.frame:frame})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(object):\n",
    "    def __init__(self, n_actions, hidden=1024, learning_rate=0.00001,\n",
    "                 frame_height=84, frame_width=84, agent_history_length=4):\n",
    "        self.n_actions = n_actions\n",
    "        self.hidden = hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.agent_history_length = agent_history_length\n",
    "        \n",
    "        self.input = tf.placeholder(shape=[None, self.frame_height,\n",
    "                                           self.frame_width, self.agent_history_length], dtype=tf.float32)\n",
    "        self.inputscaled = self.input/255\n",
    "        \n",
    "        #CONV LAYERS:\n",
    "        \n",
    "        #CONV 1:\n",
    "        self.conv1 = tf.layers.conv2d(inputs=self.inputscaled, filters=32, kernel_size=[8, 8], strides=4,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                     padding=\"valid\", activation=tf.nn.relu, use_bias=False, name=\"conv1\")\n",
    "        \n",
    "        #CONV 2:\n",
    "        self.conv2 = tf.layers.conv2d(inputs=self.conv1, filters=64, kernel_size=[4, 4], strides=2,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                     padding=\"valid\", activation=tf.nn.relu, use_bias=False, name=\"conv2\")\n",
    "        \n",
    "        #CONV 3:\n",
    "        self.conv3 = tf.layers.conv2d(inputs=self.conv2, filters=64, kernel_size=[3, 3], strides=1,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                     padding=\"valid\", activation=tf.nn.relu, use_bias=False, name=\"conv3\")\n",
    "        \n",
    "        #CONV 4:\n",
    "        self.conv4 = tf.layers.conv2d(inputs = self.conv3, filters=hidden, kernel_size=[7, 7], strides=1,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                     padding=\"valid\", activation=tf.nn.relu, use_bias=False, name=\"conv4\")\n",
    "        \n",
    "        #Split into two channels:\n",
    "        self.valuestream, self.advantagestream = tf.split(self.conv4, 2, 3)\n",
    "        \n",
    "        self.valuestream = tf.layers.flatten(self.valuestream)\n",
    "        self.advantagestream = tf.layers.flatten(self.advantagestream)\n",
    "        \n",
    "        self.advantage = tf.layers.dense(inputs=self.advantagestream, units=self.n_actions,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"advantage\")\n",
    "        \n",
    "        self.value = tf.layers.dense(inputs=self.valuestream, units=1,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"value\")\n",
    "\n",
    "        \n",
    "        #Combine the two:\n",
    "        self.q_values = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "        self.best_action = tf.argmax(self.q_values, 1)\n",
    "        \n",
    "        #target q:\n",
    "        self.target_q = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        \n",
    "        #action we took:\n",
    "        self.action = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        \n",
    "        #Q value of the action above:\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.q_values, tf.one_hot(self.action,\n",
    "                                                                     self.n_actions, dtype=tf.float32)), axis=1)\n",
    "        \n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.losses.huber_loss(labels=self.target_q, predictions= self.Q))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.update = self.optimizer.minimize(self.loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionGetter:\n",
    "    \n",
    "    def __init__(self, DQN, n_actions, eps_initial=1, eps_final=0.1, eps_final_frame=0.01,\n",
    "                 eps_evaluation=0.0, eps_annealing_frames=1000000,\n",
    "                 replay_memory_start_size=50000, max_frames = 25000000):\n",
    "        self.DQN = DQN\n",
    "        self.n_actions = n_actions\n",
    "        self.eps_initial = eps_initial\n",
    "        self.eps_final = eps_final\n",
    "        self.eps_final_frame = eps_final_frame\n",
    "        self.eps_evaluation = eps_evaluation\n",
    "        self.eps_annealing_frames = eps_annealing_frames\n",
    "        self.replay_memory_start_size = replay_memory_start_size\n",
    "        self.max_frames = max_frames\n",
    "        \n",
    "        self.slope = -(self.eps_initial - self.eps_final)/self.eps_annealing_frames\n",
    "        self.intercept = self.eps_initial - self.slope*self.replay_memory_start_size\n",
    "        self.slope_2 = -(self.eps_final - self.eps_final_frame)/(self.max_frames - self.eps_annealing_frames - self.replay_memory_start_size)\n",
    "        self.intercept_2 = self.eps_final_frame - self.slope_2*self.max_frames\n",
    "\n",
    "    def get_action(self, session, frame_number, state, evaluation=False):\n",
    "        \n",
    "        if evaluation:\n",
    "            eps = self.eps_evaluation\n",
    "        elif frame_number < self.replay_memory_start_size:\n",
    "            eps = self.eps_initial\n",
    "        elif frame_number >= self.replay_memory_start_size and frame_number < self.replay_memory_start_size + self.eps_annealing_frames:\n",
    "            eps = self.slope*frame_number + self.intercept\n",
    "        elif frame_number >= self.replay_memory_start_size + self.eps_annealing_frames:\n",
    "            eps = self.slope_2*frame_number + self.intercept_2\n",
    "        \n",
    "        if np.random.rand(1) < eps:\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "        return session.run(self.DQN.best_action, feed_dict={self.DQN.input:[state]})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, size=1000000, frame_height=84, frame_width= 84, agent_history_length=4, batch_size=32):\n",
    "        self.size = size\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.agent_history_length = agent_history_length\n",
    "        self.batch_size = batch_size\n",
    "        self.count = 0\n",
    "        self.current= 0\n",
    "        \n",
    "        #where the transitions will be stored:\n",
    "        self.actions = np.empty(self.size, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size, dtype=np.float32)\n",
    "        self.frames = np.empty((self.size, self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        self.terminal_flags = np.empty(self.size, dtype=np.bool)\n",
    "        \n",
    "        #memory for the minibatch:\n",
    "        self.states = np.empty((self.batch_size, self.agent_history_length,\n",
    "                                self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        self.new_states = np.empty((self.batch_size, self.agent_history_length,\n",
    "                                    self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        self.indices = np.empty(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "    def add_experience(self, action, frame, reward, terminal):\n",
    "        \n",
    "        if frame.shape != (self.frame_height, self.frame_width):\n",
    "            raise ValueError(\"Dimensions of frame do not match 84x84\")\n",
    "            \n",
    "        #add the experience:\n",
    "        self.actions[self.current] = action\n",
    "        self.frames[self.current, ...] = frame\n",
    "        self.rewards[self.current] = reward\n",
    "        self.terminal_flags[self.current] = terminal\n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current + 1) % self.size # if we reach the limit we start overriding the first ones\n",
    "        \n",
    "    def _get_state(self, index):\n",
    "        if self.count is 0:\n",
    "            raise ValueError(\"the memory is empty\")\n",
    "        if index < 3:\n",
    "            raise ValueError(\"index must be at least 3\")\n",
    "        return self.frames[index-self.agent_history_length+1:index+1, ...] #get the 4 frames that represent this state\n",
    "            \n",
    "    def _get_valid_indices(self):\n",
    "        for i in range(self.batch_size):\n",
    "            while True:\n",
    "                index = random.randint(self.agent_history_length, self.count - 1)\n",
    "                if index < self.agent_history_length: # index cannot be smalled than 4\n",
    "                    continue\n",
    "                if index >= self.current and index - self.history_length <= self.current: # there should be atleast 4 frames to get after the state\n",
    "                    continue\n",
    "                if self.terminal_flags[index - self.agent_history_length:index].any(): #if there is a terminal flag active, that means that in those four frame the agent died => we do not want to take them as a state\n",
    "                    continue\n",
    "                break\n",
    "            self.indices[i] = index\n",
    "        \n",
    "    def get_minibatch(self):\n",
    "            \n",
    "        if self.count < self.agent_history_length:\n",
    "            raise ValueError(\"not enough memories to get a minibatch\")\n",
    "                \n",
    "        self._get_valid_indices()\n",
    "            \n",
    "        for i, idx in enumerate(self.indices):\n",
    "            self.states[i] = self._get_state(idx - 1)\n",
    "            self.new_states[i] = self._get_state(idx)\n",
    "                \n",
    "        return np.transpose(self.states, axes=(0, 2, 3, 1)), self.actions[self.indices],self.rewards[self.indices], np.transpose(self.new_states, axes=(0, 2, 3, 1)), self.terminal_flags[self.indices]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(session, replay_memory, main_dqn, target_dqn, batch_size, gamma):\n",
    "    \n",
    "    states, actions, rewards, new_states, terminal_flags = replay_memory.get_minibatch()\n",
    "    \n",
    "    arg_q_max = session.run(main_dqn.best_action, feed_dict={main_dqn.input:new_states})\n",
    "    \n",
    "    q_vals = session.run(target_dqn.q_values, feed_dict={target_dqn.input:new_states})\n",
    "    double_q = q_vals[range(batch_size), arg_q_max]\n",
    "    \n",
    "    target_q = rewards + (gamma*double_q * (1-terminal_flags))\n",
    "    \n",
    "    loss, _= session.run([main_dqn.loss, main_dqn.update]\n",
    "                         , feed_dict={main_dqn.input:states, main_dqn.target_q:target_q, main_dqn.action:actions})\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetNetworkUpdater(object):\n",
    "    def __init__(self, main_dqn_vars, target_dqn_vars):\n",
    "        self.main_dqn_vars = main_dqn_vars\n",
    "        self.target_dqn_vars = target_dqn_vars\n",
    "        \n",
    "    def _update_target_vars(self):\n",
    "        update_ops = []\n",
    "        for i, var in enumerate(self.main_dqn_vars):\n",
    "            copy_op = self.target_dqn_vars[i].assign(var.value())\n",
    "            update_ops.append(copy_op)\n",
    "        return update_ops\n",
    "    \n",
    "    def __call__(self, sess):\n",
    "        update_ops = self._update_target_vars()\n",
    "        for copy_op in update_ops:\n",
    "            sess.run(copy_op)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atari(object):\n",
    "    \n",
    "    def __init__(self, envName, no_op_steps=10, agent_history_length=4):\n",
    "        self.env = gym.make(envName)\n",
    "        self.process_frame = FrameProcessor()\n",
    "        self.state = None\n",
    "        self.last_lives = 0\n",
    "        self.no_op_steps = no_op_steps\n",
    "        self.agent_history_length = agent_history_length\n",
    "        \n",
    "    def reset(self, sess, evaluation=False):\n",
    "        frame = self.env.reset()\n",
    "        self.last_lives = 0\n",
    "        terminal_life_lost = True\n",
    "        \n",
    "        if evaluation:\n",
    "            for _ in range(random.randint(1, self.no_op_steps)):\n",
    "                frame, _, _, _ = self.env.step(1) #fire the ball (only the first time it works)\n",
    "                \n",
    "        processed_frame = self.process_frame(sess, frame)\n",
    "        self.state = np.repeat(processed_frame, self.agent_history_length, axis=2)\n",
    "        \n",
    "        return terminal_life_lost\n",
    "    \n",
    "    def step(self, sess, action):\n",
    "        \n",
    "        new_frame, reward, terminal, info = self.env.step(action)\n",
    "        \n",
    "        if info['ale.lives'] < self.last_lives:\n",
    "            terminal_life_lost = True\n",
    "        else:\n",
    "            terminal_life_lost = terminal\n",
    "        self.last_lives = info['ale.lives']\n",
    "        \n",
    "        processed_new_frame = self.process_frame(sess, new_frame)\n",
    "        new_state = np.append(self.state[:, :, 1:], processed_new_frame, axis=2)\n",
    "        self.state = new_state\n",
    "        \n",
    "        return processed_new_frame, reward, terminal, terminal_life_lost, new_frame\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip the rewards to be -1, 0, 1. Proved to be benefitial for Breakout\n",
    "def clip_reward(reward):\n",
    "    if reward > 0:\n",
    "        return 1\n",
    "    elif reward == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#define the hyper-params:\n",
    "\n",
    "ENV_NAME = 'BreakoutDeterministic-v4'\n",
    "\n",
    "MAX_EPISODE_LENGTH = 18000\n",
    "EVAL_FREQUENCY = 200000\n",
    "EVAL_STEPS = 10000\n",
    "TARGET_NETWORK_UPDATE_FREQ = 10000\n",
    "\n",
    "DISCOUNT_FACTOR = 0.99 # Gamma in the Q learning equation\n",
    "REPLAY_MEMORY_START_SIZE = 50000 # random actions at the begining\n",
    "\n",
    "MAX_FRAMES = 10000000\n",
    "MEMORY_SIZE = 1000000\n",
    "NO_OP_STEPS = 10\n",
    "\n",
    "UPDATE_FREQ = 4\n",
    "HIDDEN = 1024\n",
    "\n",
    "LEARNING_RATE = 0.00001\n",
    "\n",
    "BS = 32\n",
    "\n",
    "#IMPORTANT FOR TRAINING OR TESTING:\n",
    "PATH = \"output/\"\n",
    "SUMMARIES = \"summaries\"\n",
    "RUNID = \"run1\"\n",
    "PATH = PATH + RUNID \n",
    "TRAIN = False\n",
    "\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "os.makedirs(os.path.join(SUMMARIES, RUNID), exist_ok=True)\n",
    "SUMM_WRITER = tf.summary.FileWriter(os.path.join(SUMMARIES, RUNID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "atari = Atari(ENV_NAME, NO_OP_STEPS)\n",
    "\n",
    "#create main DQN:\n",
    "with tf.variable_scope('mainDQN'):\n",
    "    MAIN_DQN = DDQN(atari.env.action_space.n, HIDDEN, LEARNING_RATE)\n",
    "\n",
    "#create target DQN:\n",
    "with tf.variable_scope('targetDQN'):\n",
    "    TARGET_DQN = DDQN(atari.env.action_space.n, HIDDEN)\n",
    "    \n",
    "init= tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "    \n",
    "    \n",
    "MAIN_DQN_VARS = tf.trainable_variables(scope='mainDQN')\n",
    "TARGET_DQN_VARS = tf.trainable_variables(scope='targetDQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorboard loss and reward summary:\n",
    "\n",
    "with tf.name_scope('Performace'):\n",
    "    LOSS_PH = tf.placeholder(tf.float32, shape=None, name=\"loss_summary\")\n",
    "    LOSS_SUMMARY = tf.summary.scalar(\"Loss\", LOSS_PH)\n",
    "    REWARD_PH = tf.placeholder(tf.float32, shape=None, name=\"reward_summary\")\n",
    "    REWARD_SUMMARY = tf.summary.scalar(\"reward\", REWARD_PH)\n",
    "\n",
    "PERFORMANCE_SUMMARIES = tf.summary.merge([LOSS_SUMMARY, REWARD_SUMMARY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    my_replay_memory = ReplayMemory(size=MEMORY_SIZE, batch_size=BS)   # (★)\n",
    "    update_networks = TargetNetworkUpdater(MAIN_DQN_VARS, TARGET_DQN_VARS)\n",
    "    \n",
    "    action_getter = ActionGetter(MAIN_DQN, atari.env.action_space.n, \n",
    "                                 replay_memory_start_size=REPLAY_MEMORY_START_SIZE, \n",
    "                                 max_frames=MAX_FRAMES)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        frame_number = 0\n",
    "        rewards = []\n",
    "        loss_list = []\n",
    "        \n",
    "        while frame_number < MAX_FRAMES:\n",
    "            #training:\n",
    "            epoch_frame = 0\n",
    "            print(frame_number)\n",
    "            while epoch_frame < EVAL_FREQUENCY:\n",
    "                \n",
    "                terminal_life_lost = atari.reset(sess)\n",
    "                episode_reward_sum = 0\n",
    "                \n",
    "                for _ in range(MAX_EPISODE_LENGTH):\n",
    "                    \n",
    "                    #take the action:\n",
    "                    action = action_getter.get_action(sess, frame_number, atari.state)\n",
    "                    \n",
    "                    processed_new_frame, reward, terminal, terminal_life_lost, _ = atari.step(sess, action)\n",
    "                    \n",
    "                    frame_number += 1\n",
    "                    epoch_frame += 1\n",
    "                    episode_reward_sum += reward\n",
    "                    \n",
    "                    clipped_reward = clip_reward(reward)\n",
    "                    \n",
    "                    my_replay_memory.add_experience(action=action, frame=processed_new_frame[:, :, 0], \n",
    "                                                    reward=clipped_reward, terminal=terminal_life_lost)\n",
    "                    \n",
    "                    if frame_number % UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        loss = learn(sess, my_replay_memory, MAIN_DQN, TARGET_DQN, BS, gamma = DISCOUNT_FACTOR)\n",
    "                        loss_list.append(loss)\n",
    "                    \n",
    "                    if frame_number % TARGET_NETWORK_UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        update_networks(sess)\n",
    "\n",
    "                    if terminal:  \n",
    "                        terminal = False\n",
    "                        break\n",
    "\n",
    "                rewards.append(episode_reward_sum)\n",
    "\n",
    "                if len(rewards) % 10 == 0:\n",
    "                    # summaries for tensorboard:\n",
    "                    if frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        summ = sess.run(PERFORMANCE_SUMMARIES, \n",
    "                                        feed_dict={LOSS_PH:np.mean(loss_list), \n",
    "                                                    REWARD_PH:np.mean(rewards[-100:])})\n",
    "\n",
    "                        SUMM_WRITER.add_summary(summ, frame_number)\n",
    "                        loss_list = []\n",
    "\n",
    "                    print(len(rewards), frame_number, np.mean(rewards[-100:]))\n",
    "                    with open('rewards.dat', 'a') as reward_file:\n",
    "                        print(len(rewards), frame_number, \n",
    "                              np.mean(rewards[-100:]), file=reward_file)\n",
    "\n",
    "            #evaluation:\n",
    "            \n",
    "            terminal = True\n",
    "            eval_rewards = []\n",
    "            evaluate_frame_number = 0\n",
    "            \n",
    "            for _ in range(EVAL_STEPS):\n",
    "                if terminal:\n",
    "                    terminal_life_lost = atari.reset(sess, evaluation=True)\n",
    "                    episode_reward_sum = 0\n",
    "                    terminal = False\n",
    "                    \n",
    "                action = 1 if terminal_life_lost else action_getter.get_action(sess, frame_number, atari.state,\n",
    "                                                                               evaluation = True)\n",
    "                \n",
    "                processed_new_frame, reward, terminal, terminal_life_lost, new_frame = atari.step(sess, action)\n",
    "                evaluate_frame_number += 1\n",
    "                episode_reward_sum += reward\n",
    "                \n",
    "                if terminal:\n",
    "                    eval_rewards.append(episode_reward_sum)\n",
    "                \n",
    "            print(\"Evaluation score:\\n\", np.mean(eval_rewards))\n",
    "            \n",
    "            #save network params:\n",
    "            saver.save(sess, PATH+\"/my_model\", global_step=frame_number)\n",
    "            \n",
    "            with open(\"rewardsEval.dat\", \"a\") as eval_reward_file:\n",
    "                 print(frame_number, np.mean(eval_rewards), file=eval_reward_file)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from trained/my_model-10039387\n",
      "total reward: 393.0\n"
     ]
    }
   ],
   "source": [
    "if not TRAIN:\n",
    "    action_getter = ActionGetter(MAIN_DQN, atari.env.action_space.n, \n",
    "                                 replay_memory_start_size=REPLAY_MEMORY_START_SIZE, \n",
    "                                 max_frames=MAX_FRAMES)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.import_meta_graph(\"trained/my_model-10039387.meta\")\n",
    "        saver.restore(sess,tf.train.latest_checkpoint(\"trained\"))\n",
    "        terminal_life_lost = atari.reset(sess, evaluation = True)\n",
    "        episode_reward_sum = 0\n",
    "        while True:\n",
    "            atari.env.render()\n",
    "            #time.sleep(0.02)\n",
    "            \n",
    "            action = 1 if terminal_life_lost else action_getter.get_action(sess, 0, atari.state, evaluation = True)\n",
    "                \n",
    "            processed_new_frame, reward, terminal, terminal_life_lost, new_frame = atari.step(sess ,action)\n",
    "            episode_reward_sum += reward\n",
    "            if terminal == True:\n",
    "                break\n",
    "        \n",
    "        atari.env.close()\n",
    "        print(\"total reward: {}\".format(episode_reward_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
