{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import retro\n",
    "import gym\n",
    "\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of frame:  Box(210, 160, 3)\n",
      "number of actions:  4\n",
      "actions:  [[1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "action_size = env.env.action_space.n\n",
    "possible_actions = np.array(np.identity(env.action_space.n, dtype=int).tolist())\n",
    "\n",
    "print(\"size of frame: \", env.observation_space)\n",
    "print(\"number of actions: \", action_size)\n",
    "print(\"actions: \", possible_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameProcessor(object):\n",
    "    def __init__(self, height=84, width = 84):\n",
    "        self.height = height\n",
    "        self.widht = width\n",
    "        self.frame = tf.placeholder(shape=[210,160,3], dtype=tf.unit8)\n",
    "        \n",
    "        self.gray_scaled = tf.image.rgb_to_grayscale(frame)\n",
    "        self.cropped_frame = tf.image.crop_to_bounding_box(self.gray_scaled,34, 0, 160, 160)\n",
    "        self.normalized_cropped = self.cropped_frame/255.0\n",
    "        self.preprocessed = tf.image.resize_images(self.normalized_cropped, 84, 84, method = tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "def preprocess_frame(frame, session):\n",
    "    return session.run(self.processed, feed_dict={self.frame:frame})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(object):\n",
    "    def __init__(self, number_actions, hidden=512, learning_rate=0.0000625, height=84, width=84, history_length=4):\n",
    "        self.number_actions = number_actions\n",
    "        self.hidden = hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.history_length = history_length\n",
    "        \n",
    "        self.input = tf.placeholder(shape=[None, self.height, self.width, self.history_length])\n",
    "        \n",
    "        #CONV LAYERS:\n",
    "        \n",
    "        #CONV 1:\n",
    "        self.conv1 = tf.layers.conv2d(inputs=self.input, filters=32, kernel_size=[8, 8], strides=4,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                     padding=\"valid\", activation=tf.nn.elu, use_bias=False, name=\"conv1\")\n",
    "        \n",
    "        #CONV 2:\n",
    "        self.conv2 = tf.layers.conv2d(inputs=self.conv1, filters=64, kernel_size=[4, 4], strides=2,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                     padding=\"valid\", activation=tf.nn.elu, use_bias=False, name=\"conv2\")\n",
    "        \n",
    "        #CONV 3:\n",
    "        self.conv3 = tf.layers.conv2d(inputs=self.conv2, filters=64, kernel_size=[3, 3], strides=1,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                     padding=\"valid\", activation=tf.nn.elu, use_bias=False, name=\"conv3\")\n",
    "        \n",
    "        self.flattern = tf.layers.flatten(self.conv3)\n",
    "        \n",
    "        \n",
    "        #Calculate V(s)\n",
    "        self.value_fc = tf.layers.dense(inputs = self.flatten, units=hidden, activation=tf.nn.elu,\n",
    "                                        kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"value_fc\")\n",
    "        \n",
    "        self.value = tf.layers.dense(inputs = self.value_fc, units=1, activation=None,\n",
    "                                        kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"value\")\n",
    "        \n",
    "        #Calculate A(s,a)\n",
    "        self.advantage_fc = tf.layers.dense(inputs = self.flatten, units=hidden, activation=tf.nn.elu,\n",
    "                                        kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"advantage_fc\")\n",
    "        \n",
    "        self.advantage = tf.layers.dense(inputs = self.advantage_fc, units=self.number_actions, activation=None,\n",
    "                                        kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"advantages\")\n",
    "\n",
    "        \n",
    "        #Combine the two:\n",
    "        self.q_values = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "        self.best_action = tf.argmax(self.q_values, 1)\n",
    "        \n",
    "        #target q:\n",
    "        self.target_q = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        \n",
    "        #action we took:\n",
    "        self.action = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        \n",
    "        #Q value of the action above:\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.q_values, tf.one_hot(self.action, self.number_actions, dtype=tf.float32))\n",
    "                               , axis=1)\n",
    "        \n",
    "        \n",
    "        self.loss = tf.reduce.mean(tf.losses.huber_loss(labels=self.target_q, predictions= self.Q))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.update = self.optimizer.minimize(self.loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions, session, DDQN):\n",
    "    exp_tradeoff = np.random.rand()\n",
    "    \n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if(explore_probability > exp_tradeoff):\n",
    "        #random action(exploration)\n",
    "        action = np.random.randint(0, action_size)\n",
    "    else:\n",
    "        action = session.run(DDQN.best_action, feed_dict={DDQN.input:[state]})[0]\n",
    "        \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReplayMemory(object):\n",
    "    def __init__(self, size= 100000, height=84, width= 84, history_length=4, batch_size=32):\n",
    "        self.size = size\n",
    "        self.hight = height\n",
    "        self.history_length = history_length\n",
    "        self.batch_size = batch_size\n",
    "        self.count = 0\n",
    "        self.current= 0\n",
    "        \n",
    "        #where the transitions will be stored:\n",
    "        self.actions = np.empty(self.size, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size, dtype=np.int32)\n",
    "        self.frames = np.empty((self.size, self.height, self.width), dtype=np.uint8)\n",
    "        self.terminal_flags = np.empty(self.size, dtype=np.int32)\n",
    "        \n",
    "        #memory for the minibatch:\n",
    "        self.states = np.empty((self.batch_size, self.history_length, self.frame_height, self.frame_width),\n",
    "                               dtype=np.uint8)\n",
    "        self.new_states = np.empty((self.batch_size, self.history_length, self.frame_height, self.frame_width),\n",
    "                               dtype=np.uint8)\n",
    "        self.indices = np.empty(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "        def add_experience(self, action, frame, reward, terminal):\n",
    "            if frame.shape != (self.height, self.width):\n",
    "                raise ValueError(\"Dimensions of frame do not match 84x84\")\n",
    "            \n",
    "            #add the experience:\n",
    "            self.actions[self.current] = action\n",
    "            self.frames[self.current, ...] = frame\n",
    "            self.rewards[self.current] = reward\n",
    "            self.terminal_flags[self.current] = terminal\n",
    "            self.count = max(self.count, self.current+1)\n",
    "            self.current = (self.current + 1) % self.size # if we reach the limit we start overriding the first ones\n",
    "        \n",
    "        def _get_state(self, index):\n",
    "            if self.count is 0:\n",
    "                raise ValueError(\"the memory is empty\")\n",
    "            if index < 3:\n",
    "                raise ValueError(\"index must be at least 3\")\n",
    "            return self.frames[index-self.history_length+1:index+1, ...] #get the 4 frames that represent this state\n",
    "            \n",
    "        def _get_valid_indices(self):\n",
    "            for i in range(self.batch_size):\n",
    "                while True:\n",
    "                    index = random.randint(self.history_length, self.count - 1)\n",
    "                    if index < self.history_length: # index cannot be smalled than 4\n",
    "                        continue\n",
    "                    if index >= self.current and index - self.history_length <= self.current: # there should be atleast 4 frames to get after the state\n",
    "                        continue\n",
    "                    if self.terminal_flags[index - self.history_length:index].any(): #if there is a terminal flag active, that means that in those four frame the agent died => we do not want to take them as a state\n",
    "                        continue\n",
    "                    break\n",
    "                self.indices[i] = index\n",
    "        \n",
    "        def get_minibatch(self):\n",
    "            \n",
    "            if self.count < self.history_length:\n",
    "                raise ValueError(\"not enough memories to get a minibatch\")\n",
    "                \n",
    "            self._get_valid_indices()\n",
    "            \n",
    "            for i, idx in enumerate(self.indices):\n",
    "                self.states[i] = self._get_state(idx - 1)\n",
    "                self.new_states[i] = self._get_state(idx)\n",
    "                \n",
    "            return np.transpose(self.states, axes=(0,2,3,1)), self.actions[self.indices], self.rewards[self.indices], self.transpose(self.new_states, axes(0,2,3,1)), self.terminal_flags[self.indices]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
