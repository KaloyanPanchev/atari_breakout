{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import retro\n",
    "import gym\n",
    "import os\n",
    "\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "#remove deprication msgs for .layers\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of frame:  Box(210, 160, 3)\n",
      "number of actions:  4\n",
      "actions:  ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "ACTION_SIZE = env.env.action_space.n\n",
    "possible_actions = env.unwrapped.get_action_meanings()\n",
    "\n",
    "print(\"size of frame: \", env.observation_space)\n",
    "print(\"number of actions: \", ACTION_SIZE)\n",
    "print(\"actions: \", possible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 4\n",
      "0.0 False 4\n",
      "0.0 False 4\n",
      "0.0 False 4\n",
      "0.0 False 4\n",
      "0.0 False 4\n"
     ]
    }
   ],
   "source": [
    "#try a few actions and games:\n",
    "frame = env.reset()\n",
    "for i in range(30):\n",
    "    new_frame, reward, terminal, info = env.step(1)\n",
    "    print(reward, terminal, info['ale.lives'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameProcessor(object):\n",
    "    def __init__(self, height=84, width = 84):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.frame = tf.placeholder(shape=[210,160,3], dtype=tf.uint8)\n",
    "        self.processed = tf.image.rgb_to_grayscale(self.frame)\n",
    "        self.processed = tf.image.crop_to_bounding_box(self.processed,34, 0, 160, 160)\n",
    "        self.processed = tf.image.resize_images(self.processed, [self.height, self.width] , method = tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    def process(self, session, frame):\n",
    "        return session.run(self.processed, feed_dict={self.frame:frame})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(object):\n",
    "    def __init__(self, number_actions, hidden=1024, learning_rate=0.0000625, height=84, width=84, history_length=4):\n",
    "        self.number_actions = number_actions\n",
    "        self.hidden = hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.history_length = history_length\n",
    "        \n",
    "        self.input = tf.placeholder(shape=[None, self.height, self.width, self.history_length], dtype=tf.float32)\n",
    "        self.input = self.input/255\n",
    "        \n",
    "        #CONV LAYERS:\n",
    "        \n",
    "        #CONV 1:\n",
    "        self.conv1 = tf.layers.conv2d(inputs=self.input, filters=32, kernel_size=[8, 8], strides=4,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                     padding=\"valid\", activation=tf.nn.relu, use_bias=False, name=\"conv1\")\n",
    "        \n",
    "        #CONV 2:\n",
    "        self.conv2 = tf.layers.conv2d(inputs=self.conv1, filters=64, kernel_size=[4, 4], strides=2,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                     padding=\"valid\", activation=tf.nn.relu, use_bias=False, name=\"conv2\")\n",
    "        \n",
    "        #CONV 3:\n",
    "        self.conv3 = tf.layers.conv2d(inputs=self.conv2, filters=64, kernel_size=[3, 3], strides=1,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                     padding=\"valid\", activation=tf.nn.relu, use_bias=False, name=\"conv3\")\n",
    "        \n",
    "        #CONV 4:\n",
    "        self.conv4 = tf.layers.conv2d(inputs = self.conv3, filters=hidden, kernel_size=[7, 7], strides=1,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                     padding=\"valid\", activation=tf.nn.relu, use_bias=False, name=\"conv4\")\n",
    "        \n",
    "        #Split into two channels:\n",
    "        self.valuestream, self.advantagestream = tf.split(self.conv4, 2, 3)\n",
    "        \n",
    "        #value stream:\n",
    "        self.valuestream = tf.layers.flatten(self.valuestream)\n",
    "        self.value = tf.layers.dense(inputs=self.valuestream, units=1,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"value\")\n",
    "        \n",
    "        #advantage stream:\n",
    "        self.advantagestream = tf.layers.flatten(self.advantagestream)\n",
    "        self.advantage = tf.layers.dense(inputs=self.advantagestream, units=1,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"advantage\")\n",
    "\n",
    "        \n",
    "        #Combine the two:\n",
    "        self.q_values = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "        self.best_action = tf.argmax(self.q_values, 1)\n",
    "        \n",
    "        #target q:\n",
    "        self.target_q = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        \n",
    "        #action we took:\n",
    "        self.action = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        \n",
    "        #Q value of the action above:\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.q_values, tf.one_hot(self.action,\n",
    "                                                                     self.number_actions, dtype=tf.float32)), axis=1)\n",
    "        \n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.losses.huber_loss(labels=self.target_q, predictions= self.Q))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.update = self.optimizer.minimize(self.loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionGetter:\n",
    "    \n",
    "    def __init__(self, n_actions, eps_initial=1, eps_final=0.1, eps_final_frame=0.01,\n",
    "                 eps_evaluation=0.0, eps_annealing_frames=1000000,\n",
    "                 replay_memory_start_size=50000, max_frames = 1000000):\n",
    "        self.n_actions = n_actions\n",
    "        self.eps_initial = eps_initial\n",
    "        self.eps_final = eps_final\n",
    "        self.eps_final_frame = eps_final_frame\n",
    "        self.eps_evaluation = eps_evaluation\n",
    "        self.eps_annealing_frames = eps_annealing_frames\n",
    "        self.replay_memory_start_size = replay_memory_start_size\n",
    "        self.max_frames = max_frames\n",
    "        \n",
    "        self.slope = -(self.eps_initial - self.eps_final)/self.eps_annealing_frames\n",
    "        self.intercept = self.eps_initial - self.slope*self.replay_memory_start_size\n",
    "        self.slope_2 = -(self.eps_final - self.eps_final_frame)/(self.max_frames - self.eps_annealing_frames - self.replay_memory_start_size)\n",
    "        self.intercept_2 = self.eps_final_frame - self.slope_2*self.max_frames\n",
    "\n",
    "    def get_action(self, session, frame_number, state, main_ddqn, evaluation=False):\n",
    "        \n",
    "        if evaluation:\n",
    "            eps = self.eps_evaluation\n",
    "        elif frame_number < self.replay_memory_start_size:\n",
    "            eps = self.eps_initial\n",
    "        elif frame_number >= self.replay_memory_start_size and frame_number < self.replay_memory_start_size + self.eps_annealing_frames:\n",
    "            eps = self.slope*frame_number + self.intercept\n",
    "        elif frame_number >= self.replay_memory_start_size + self.eps_annealing_frames:\n",
    "            eps = self.slope_2*frame_number + self.intercept_2\n",
    "        \n",
    "        if np.random.rand(1) < eps:\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "        return session.run(main_ddqn.best_action, feed_dict={main_ddqn.input:[state]})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, size=100000, height=84, width= 84, history_length=4, batch_size=32):\n",
    "        self.size = size\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.history_length = history_length\n",
    "        self.batch_size = batch_size\n",
    "        self.count = 0\n",
    "        self.current= 0\n",
    "        \n",
    "        #where the transitions will be stored:\n",
    "        self.actions = np.empty(self.size, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size, dtype=np.int32)\n",
    "        self.frames = np.empty((self.size, self.height, self.width), dtype=np.uint8)\n",
    "        self.terminal_flags = np.empty(self.size, dtype=np.bool)\n",
    "        \n",
    "        #memory for the minibatch:\n",
    "        self.states = np.empty((self.batch_size, self.history_length, self.height, self.width),\n",
    "                               dtype=np.uint8)\n",
    "        self.new_states = np.empty((self.batch_size, self.history_length, self.height, self.width),\n",
    "                               dtype=np.uint8)\n",
    "        self.indices = np.empty(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "    def add_experience(self, action, frame, reward, terminal):\n",
    "        if frame.shape != (self.height, self.width):\n",
    "            raise ValueError(\"Dimensions of frame do not match 84x84\")\n",
    "            \n",
    "        #add the experience:\n",
    "        self.actions[self.current] = action\n",
    "        self.frames[self.current, ...] = frame\n",
    "        self.rewards[self.current] = reward\n",
    "        self.terminal_flags[self.current] = terminal\n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current + 1) % self.size # if we reach the limit we start overriding the first ones\n",
    "        \n",
    "    def _get_state(self, index):\n",
    "        if self.count is 0:\n",
    "            raise ValueError(\"the memory is empty\")\n",
    "        if index < 3:\n",
    "            raise ValueError(\"index must be at least 3\")\n",
    "        return self.frames[index-self.history_length+1:index+1, ...] #get the 4 frames that represent this state\n",
    "            \n",
    "    def _get_valid_indices(self):\n",
    "        for i in range(self.batch_size):\n",
    "            while True:\n",
    "                index = random.randint(self.history_length, self.count - 1)\n",
    "                if index < self.history_length: # index cannot be smalled than 4\n",
    "                    continue\n",
    "                if index >= self.current and index - self.history_length <= self.current: # there should be atleast 4 frames to get after the state\n",
    "                    continue\n",
    "                if self.terminal_flags[index - self.history_length:index].any(): #if there is a terminal flag active, that means that in those four frame the agent died => we do not want to take them as a state\n",
    "                    continue\n",
    "                break\n",
    "            self.indices[i] = index\n",
    "        \n",
    "    def get_minibatch(self):\n",
    "            \n",
    "        if self.count < self.history_length:\n",
    "            raise ValueError(\"not enough memories to get a minibatch\")\n",
    "                \n",
    "        self._get_valid_indices()\n",
    "            \n",
    "        for i, idx in enumerate(self.indices):\n",
    "            self.states[i] = self._get_state(idx - 1)\n",
    "            self.new_states[i] = self._get_state(idx)\n",
    "                \n",
    "        return np.transpose(self.states, axes=(0,2,3,1)), self.actions[self.indices], self.rewards[self.indices], np.transpose(self.new_states, axes=(0,2,3,1)), self.terminal_flags[self.indices]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(session, replay_memory, main_ddqn, target_ddqn, batch_size, gamma):\n",
    "    states, actions, rewards, new_states, terminal_flags = replay_memory.get_minibatch()\n",
    "    \n",
    "    best_new_actions = session.run(main_ddqn.best_action, feed_dict={main_ddqn.input:new_states})\n",
    "    \n",
    "    q_vals = session.run(target_ddqn.q_values, feed_dict={target_ddqn.input:new_states})\n",
    "    double_q = q_vals[range(batch_size), best_new_actions]\n",
    "    \n",
    "    target_q = rewards + (gamma*double_q * (1 - terminal_flags))\n",
    "    \n",
    "    loss, _= session.run([main_ddqn.loss, main_ddqn.update]\n",
    "                         , feed_dict={main_ddqn.input:states, main_ddqn.target_q:target_q, main_ddqn.action:actions})\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetNetworkUpdater(object):\n",
    "    def __init__(self, main_ddqn_vars, target_ddqn_vars):\n",
    "        self.main_ddqn_vars = main_ddqn_vars\n",
    "        self.target_ddqn_vars = target_ddqn_vars\n",
    "        \n",
    "    def _update_target_vars(self):\n",
    "        update_ops = []\n",
    "        for i, var in enumerate(self.main_ddqn_vars):\n",
    "            copy_op = self.target_ddqn_vars[i].assign(var.value())\n",
    "            update_ops.append(copy_op)\n",
    "        return update_ops\n",
    "    \n",
    "    def __call__(self, sess):\n",
    "        update_ops = self._update_target_vars()\n",
    "        for copy_op in update_ops:\n",
    "            sess.run(copy_op)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreakoutGame(object):\n",
    "    \n",
    "    def __init__(self, env_name=\"BreakoutDeterministic-v4\", no_op_steps=10, history_length=4):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.frame_processor = FrameProcessor()\n",
    "        self.state = None\n",
    "        self.last_lives = 0\n",
    "        self.no_op_steps = no_op_steps\n",
    "        self.history_length = history_length\n",
    "        \n",
    "    def reset(self, sess, evaluation=False):\n",
    "        frame = self.env.reset()\n",
    "        self.last_lives = 0\n",
    "        terminal_life_lost = True\n",
    "        \n",
    "        if evaluation:\n",
    "            for _ in range(random.randint(1, self.no_op_steps)):\n",
    "                frame, _, _, _ = self.env.step(1) #fire the ball (only the first time it works)\n",
    "                \n",
    "        processed_frame = self.frame_processor.process(sess, frame)\n",
    "        self.state = np.repeat(processed_frame, self.history_length, axis=2)\n",
    "        \n",
    "        return terminal_life_lost\n",
    "    \n",
    "    def step(self, sess, action):\n",
    "        \n",
    "        new_frame, reward, terminal, info = self.env.step(action)\n",
    "        \n",
    "        if info['ale.lives'] < self.last_lives:\n",
    "            terminal_life_lost = True\n",
    "        else:\n",
    "            terminal_life_lost = terminal\n",
    "        self.last_lives = info['ale.lives']\n",
    "        \n",
    "        processed_new_frame = self.frame_processor.process(sess, new_frame)\n",
    "        new_state = np.append(self.state[:, :, 1:], processed_new_frame, axis=2)\n",
    "        self.state = new_state\n",
    "        \n",
    "        return processed_new_frame, reward, terminal, terminal_life_lost, new_frame\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip the rewards to be -1, 0, 1. Proved to be benefitial for Breakout\n",
    "def clip_reward(reward):\n",
    "    if reward > 0:\n",
    "        return 1\n",
    "    elif reward == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#define the hyper-params:\n",
    "\n",
    "ENV_NAME = 'BreakoutDeterministic-v4'\n",
    "\n",
    "MAX_EPISODE_LENGTH = 18000\n",
    "EVAL_FREQUENCY = 200000\n",
    "EVAL_STEPS = 10000\n",
    "TARGET_NETWORK_UPDATE_FREQ = 10000\n",
    "\n",
    "DISCOUNT_FACTOR = 0.99 # Gamma in the Q learning equation\n",
    "REPLAY_MEMORY_START_SIZE = 50000 # random actions at the begining\n",
    "\n",
    "MAX_FRAMES = 7000000\n",
    "MEMORY_SIZE = 1000000\n",
    "NO_OP_STEPS = 10\n",
    "\n",
    "UPDATE_FREQ = 4\n",
    "HIDDEN = 1024\n",
    "\n",
    "LEARNING_RATE = 0.0000625\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "EXPLORE_START = 1.0\n",
    "EXPLORE_STOP = 0.01\n",
    "DECAY_RATE = 0.00005\n",
    "\n",
    "#IMPORTANT FOR TRAINING OR TESTING:\n",
    "PATH = \"output/\"\n",
    "INFO = \"info\"\n",
    "RUNID = \"run1\"\n",
    "PATH = PATH + RUNID \n",
    "TRAIN = True\n",
    "\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "os.makedirs(os.path.join(INFO, RUNID), exist_ok=True)\n",
    "SUMM_WRITER = tf.summary.FileWriter(os.path.join(INFO, RUNID))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "atari = BreakoutGame(ENV_NAME, NO_OP_STEPS)\n",
    "\n",
    "#create main DQN:\n",
    "with tf.variable_scope('mainDDQN'):\n",
    "    MAIN_DDQN = DDQN(atari.env.action_space.n, HIDDEN, LEARNING_RATE)\n",
    "\n",
    "MAIN_DDQN_VARS = tf.trainable_variables(scope='mainDDQN')\n",
    "\n",
    "\n",
    "#create target DQN:\n",
    "with tf.variable_scope('targetDDQN'):\n",
    "    TARGET_DDQN = DDQN(atari.env.action_space.n, HIDDEN)\n",
    "\n",
    "TARGET_DDQN_VARS = tf.trainable_variables(scope='targetDDQN')\n",
    "\n",
    "\n",
    "init= tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS_IDS = [\"conv1\", \"conv2\", \"conv3\", \"value_fc\", \"value\", \"advantage_fc\", \"advantages\"]\n",
    "\n",
    "with tf.name_scope(\"Performace\"):\n",
    "    LOSS_PH = tf.placeholder(tf.float32, shape=None, name=\"loss_summary\")\n",
    "    LOSS_SUMMARY = tf.summary.scalar(\"loss\", LOSS_PH)\n",
    "    REWARD_PH = tf.placeholder(tf.float32, shape=None, name=\"reward_summary\")\n",
    "    REWARD_SUMMARY = tf.summary.scalar(\"reward\", REWARD_PH)\n",
    "    EVAL_SCORE_PH = tf.placeholder(tf.float32, shape=None, name=\"eval_summary\")\n",
    "    EVAL_SCORE_SUMMARY = tf.summary.scalar(\"eval_score\", EVAL_SCORE_PH)\n",
    "    \n",
    "PERFORMANCE_SUMMARIES = tf.summary.merge([LOSS_SUMMARY, REWARD_SUMMARY])\n",
    "\n",
    "with tf.name_scope(\"Parameters\"):\n",
    "    ALL_PARAM_SUMMARIES = []\n",
    "    for i, Id in enumerate(LAYERS_IDS):\n",
    "        with tf.name_scope(\"mainDDQN/\"):\n",
    "            MAIN_DDQN_KERNEL = tf.summary.histogram(Id, tf.reshape(MAIN_DDQN_VARS[i], shape=[-1]))\n",
    "        ALL_PARAM_SUMMARIES.extend([MAIN_DDQN_KERNEL])\n",
    "PARAM_SUMMARIES = tf.summary.merge(ALL_PARAM_SUMMARIES)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    my_replay_memory = ReplayMemory(size=MEMORY_SIZE, batch_size=BATCH_SIZE)   # (â˜…)\n",
    "    network_updater = TargetNetworkUpdater(MAIN_DDQN_VARS, TARGET_DDQN_VARS)\n",
    "    action_getter = ActionGetter(atari.env.action_space.n, \n",
    "                                 replay_memory_start_size=REPLAY_MEMORY_START_SIZE, \n",
    "                                 max_frames=MAX_FRAMES)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        frame_number = 0\n",
    "        rewards = []\n",
    "        loss_list = []\n",
    "        \n",
    "        while frame_number < MAX_FRAMES:\n",
    "            #training:\n",
    "            epoch_frame = 0\n",
    "            print(frame_number)\n",
    "            while epoch_frame < EVAL_FREQUENCY:\n",
    "                \n",
    "                terminal_life_lost = atari.reset(sess)\n",
    "                episode_reward_sum = 0\n",
    "                \n",
    "                for _ in range(MAX_EPISODE_LENGTH):\n",
    "                    \n",
    "                    #take the action:\n",
    "                    action = action_getter.get_action(sess, frame_number, atari.state, MAIN_DDQN)\n",
    "                    \n",
    "                    processed_new_frame, reward, terminal, terminal_life_lost, _ = atari.step(sess, action)\n",
    "                    \n",
    "                    frame_number += 1\n",
    "                    epoch_frame += 1\n",
    "                    episode_reward_sum += reward\n",
    "                    \n",
    "                    clipped_reward = clip_reward(reward)\n",
    "                    \n",
    "                    my_replay_memory.add_experience(action=action, frame=processed_new_frame[:, :, 0], reward=clipped_reward,\n",
    "                                                   terminal=terminal_life_lost)\n",
    "                    \n",
    "                    if frame_number % UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        loss = learn(sess, my_replay_memory, MAIN_DDQN, TARGET_DDQN, BATCH_SIZE, gamma = DISCOUNT_FACTOR)\n",
    "                    \n",
    "                    if frame_number % TARGET_NETWORK_UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        network_updater.update_networks(sess)\n",
    "\n",
    "                    if terminal:  \n",
    "                        terminal = False\n",
    "                        break\n",
    "\n",
    "                rewards.append(episode_reward_sum)\n",
    "\n",
    "                if len(rewards) % 10 == 0:\n",
    "                    # Scalar summaries for tensorboard\n",
    "                    if frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        summ = sess.run(PERFORMANCE_SUMMARIES, \n",
    "                                        feed_dict={LOSS_PH:np.mean(loss_list), \n",
    "                                                    REWARD_PH:np.mean(rewards[-100:])})\n",
    "\n",
    "                        SUMM_WRITER.add_summary(summ, frame_number)\n",
    "                        loss_list = []\n",
    "                        # Histogramm summaries for tensorboard\n",
    "                    summ_param = sess.run(PARAM_SUMMARIES)\n",
    "                    SUMM_WRITER.add_summary(summ_param, frame_number)\n",
    "\n",
    "                    print(len(rewards), frame_number, np.mean(rewards[-100:]))\n",
    "                    with open('rewards.dat', 'a') as reward_file:\n",
    "                        print(len(rewards), frame_number, \n",
    "                              np.mean(rewards[-100:]), file=reward_file)\n",
    "\n",
    "            #evaluation:\n",
    "            \n",
    "            terminal = True\n",
    "            eval_rewards = []\n",
    "            evaluate_frame_number = 0\n",
    "            \n",
    "            for _ in range(EVAL_STEPS):\n",
    "                if terminal:\n",
    "                    terminal_life_lost = atari.reset(sess, evaluation=True)\n",
    "                    episode_reward_sum = 0\n",
    "                    terminal = False\n",
    "                    \n",
    "                action = 1 if terminal_life_lost else action_getter.get_action(sess, frame_number, atari.state,\n",
    "                                                                               MAIN_DDQN, evaluation = True)\n",
    "                \n",
    "                processed_new_frame, reward, terminal, terminal_life_lost, _ = atari.step(sess, action)\n",
    "                evaluate_frame_number += 1\n",
    "                episode_reward_sum += reward\n",
    "                \n",
    "                if terminal:\n",
    "                    eval_rewards.append(episode_reward_sum)\n",
    "                \n",
    "            print(\"Evaluation score:\\n\", np.mean(eval_rewards))\n",
    "            \n",
    "            #save network params:\n",
    "            saver.save(sess, PATH+\"/my_model\", global_step=frame_number)\n",
    "            \n",
    "            summ = sess.run(EVAL_SCORE_SUMMARY, feed_dict={EVAL_SCORE_PH:np.mean(eval_rewards)})\n",
    "            SUMM_WRITER.add_summary(summ, frame_number)\n",
    "            \n",
    "            with open(\"rewardsEval.dat\", \"a\") as eval_reward_file:\n",
    "                 print(frame_number, np.mean(eval_rewards), file=eval_reward_file)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10 1635 0.7\n",
      "20 3446 0.95\n",
      "30 5214 0.9666666666666667\n",
      "40 7194 1.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-831e35bdf792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-e8a7bb299502>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_getter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matari\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAIN_DDQN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                     \u001b[0mprocessed_new_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal_life_lost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matari\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0mframe_number\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-bc6e49a53a7f>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, sess, action)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mnew_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ale.lives'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_lives\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/final_project/final_project/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/final_project/final_project/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/final_project/final_project/lib/python3.6/site-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN:\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.import_meta_graph(\"trained/my_model-4005245.meta\")\n",
    "        saver.restore(sess,tf.train.latest_checkpoint(\"trained\"))\n",
    "        terminal_life_lost = atari.reset(sess, evaluation = True)\n",
    "        episode_reward_sum = 0\n",
    "        while True:\n",
    "            atari.env.render()\n",
    "            \n",
    "            action = 1 if terminal_life_lost else action_getter.get_action(sess, frame_number, atari.state,\n",
    "                                                                           MAIN_DDQN, evaluation = True)\n",
    "                \n",
    "            processed_new_frame, reward, terminal, terminal_life_lost, new_frame = atari.step(sess ,action)\n",
    "            episode_reward_sum += reward\n",
    "            if terminal == True:\n",
    "                break\n",
    "        \n",
    "        atari.env.close()\n",
    "        print(\"total reward: {}\".format(episode_reward_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
