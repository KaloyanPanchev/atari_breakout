{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import retro\n",
    "import gym\n",
    "import os\n",
    "\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "#remove deprication msgs for .layers\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of frame:  Box(210, 160, 3)\n",
      "number of actions:  4\n",
      "actions:  ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "ACTION_SIZE = env.env.action_space.n\n",
    "possible_actions = env.unwrapped.get_action_meanings()\n",
    "\n",
    "print(\"size of frame: \", env.observation_space)\n",
    "print(\"number of actions: \", ACTION_SIZE)\n",
    "print(\"actions: \", possible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 5\n",
      "0.0 False 4\n",
      "0.0 False 4\n",
      "0.0 False 4\n",
      "0.0 False 4\n",
      "0.0 False 4\n",
      "0.0 False 4\n"
     ]
    }
   ],
   "source": [
    "#try a few actions and games:\n",
    "frame = env.reset()\n",
    "for i in range(30):\n",
    "    new_frame, reward, terminal, info = env.step(1)\n",
    "    print(reward, terminal, info['ale.lives'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameProcessor(object):\n",
    "    def __init__(self, height=84, width = 84):\n",
    "        self.height = height\n",
    "        self.widht = width\n",
    "        self.frame = tf.placeholder(shape=[210,160,3], dtype=tf.uint8)\n",
    "        \n",
    "        self.gray_scaled = tf.image.rgb_to_grayscale(frame)\n",
    "        self.cropped_frame = tf.image.crop_to_bounding_box(self.gray_scaled,34, 0, 160, 160)\n",
    "        self.processed = tf.image.resize_images(self.cropped_frame, [84, 84] , method = tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    def __call__(self, session, frame):\n",
    "        return session.run(self.processed, feed_dict={self.frame:frame})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(object):\n",
    "    def __init__(self, number_actions, hidden=512, learning_rate=0.0000625, height=84, width=84, history_length=4):\n",
    "        self.number_actions = number_actions\n",
    "        self.hidden = hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.history_length = history_length\n",
    "        \n",
    "        self.input = tf.placeholder(shape=[None, self.height, self.width, self.history_length], dtype=tf.float32)\n",
    "        self.input = self.input/255\n",
    "        \n",
    "        #CONV LAYERS:\n",
    "        \n",
    "        #CONV 1:\n",
    "        self.conv1 = tf.layers.conv2d(inputs=self.input, filters=32, kernel_size=[8, 8], strides=4,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                     padding=\"valid\", activation=tf.nn.elu, use_bias=False, name=\"conv1\")\n",
    "        \n",
    "        #CONV 2:\n",
    "        self.conv2 = tf.layers.conv2d(inputs=self.conv1, filters=64, kernel_size=[4, 4], strides=2,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                     padding=\"valid\", activation=tf.nn.elu, use_bias=False, name=\"conv2\")\n",
    "        \n",
    "        #CONV 3:\n",
    "        self.conv3 = tf.layers.conv2d(inputs=self.conv2, filters=64, kernel_size=[3, 3], strides=1,\n",
    "                                     kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
    "                                     padding=\"valid\", activation=tf.nn.elu, use_bias=False, name=\"conv3\")\n",
    "        \n",
    "        self.flatten = tf.layers.flatten(self.conv3)\n",
    "        \n",
    "        \n",
    "        #Calculate V(s)\n",
    "        self.value_fc = tf.layers.dense(inputs = self.flatten, units=hidden, activation=tf.nn.elu,\n",
    "                                        kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"value_fc\")\n",
    "        \n",
    "        self.value = tf.layers.dense(inputs = self.value_fc, units=1, activation=None,\n",
    "                                        kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"value\")\n",
    "        \n",
    "        #Calculate A(s,a)\n",
    "        self.advantage_fc = tf.layers.dense(inputs = self.flatten, units=hidden, activation=tf.nn.elu,\n",
    "                                        kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"advantage_fc\")\n",
    "        \n",
    "        self.advantage = tf.layers.dense(inputs = self.advantage_fc, units=self.number_actions, activation=None,\n",
    "                                        kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"advantages\")\n",
    "\n",
    "        \n",
    "        #Combine the two:\n",
    "        self.q_values = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "        self.best_action = tf.argmax(self.q_values, 1)\n",
    "        \n",
    "        #target q:\n",
    "        self.target_q = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        \n",
    "        #action we took:\n",
    "        self.action = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        \n",
    "        #Q value of the action above:\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.q_values, tf.one_hot(self.action, self.number_actions, dtype=tf.float32))\n",
    "                               , axis=1)\n",
    "        \n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.losses.huber_loss(labels=self.target_q, predictions= self.Q))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.update = self.optimizer.minimize(self.loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(session, DDQN, explore_start, explore_stop, decay_rate, decay_step, state, action_size, evaluation = False):\n",
    "    exp_tradeoff = np.random.rand()\n",
    "    \n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if evaluation:\n",
    "        explore_probability = 0.0\n",
    "    \n",
    "    if(explore_probability > exp_tradeoff):\n",
    "        #random action(exploration)\n",
    "        action = np.random.randint(0, action_size)\n",
    "    else:\n",
    "        action = session.run(DDQN.best_action, feed_dict={DDQN.input:[state]})[0]\n",
    "        \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, size=100000, height=84, width= 84, history_length=4, batch_size=32):\n",
    "        self.size = size\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.history_length = history_length\n",
    "        self.batch_size = batch_size\n",
    "        self.count = 0\n",
    "        self.current= 0\n",
    "        \n",
    "        #where the transitions will be stored:\n",
    "        self.actions = np.empty(self.size, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size, dtype=np.int32)\n",
    "        self.frames = np.empty((self.size, self.height, self.width), dtype=np.uint8)\n",
    "        self.terminal_flags = np.empty(self.size, dtype=np.int32)\n",
    "        \n",
    "        #memory for the minibatch:\n",
    "        self.states = np.empty((self.batch_size, self.history_length, self.height, self.width),\n",
    "                               dtype=np.uint8)\n",
    "        self.new_states = np.empty((self.batch_size, self.history_length, self.height, self.width),\n",
    "                               dtype=np.uint8)\n",
    "        self.indices = np.empty(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "    def add_experience(self, action, frame, reward, terminal):\n",
    "        if frame.shape != (self.height, self.width):\n",
    "            raise ValueError(\"Dimensions of frame do not match 84x84\")\n",
    "            \n",
    "        #add the experience:\n",
    "        self.actions[self.current] = action\n",
    "        self.frames[self.current, ...] = frame\n",
    "        self.rewards[self.current] = reward\n",
    "        self.terminal_flags[self.current] = terminal\n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current + 1) % self.size # if we reach the limit we start overriding the first ones\n",
    "        \n",
    "    def _get_state(self, index):\n",
    "        if self.count is 0:\n",
    "            raise ValueError(\"the memory is empty\")\n",
    "        if index < 3:\n",
    "            raise ValueError(\"index must be at least 3\")\n",
    "        return self.frames[index-self.history_length+1:index+1, ...] #get the 4 frames that represent this state\n",
    "            \n",
    "    def _get_valid_indices(self):\n",
    "        for i in range(self.batch_size):\n",
    "            while True:\n",
    "                index = random.randint(self.history_length, self.count - 1)\n",
    "                if index < self.history_length: # index cannot be smalled than 4\n",
    "                    continue\n",
    "                if index >= self.current and index - self.history_length <= self.current: # there should be atleast 4 frames to get after the state\n",
    "                    continue\n",
    "                if self.terminal_flags[index - self.history_length:index].any(): #if there is a terminal flag active, that means that in those four frame the agent died => we do not want to take them as a state\n",
    "                    continue\n",
    "                break\n",
    "            self.indices[i] = index\n",
    "        \n",
    "    def get_minibatch(self):\n",
    "            \n",
    "        if self.count < self.history_length:\n",
    "            raise ValueError(\"not enough memories to get a minibatch\")\n",
    "                \n",
    "        self._get_valid_indices()\n",
    "            \n",
    "        for i, idx in enumerate(self.indices):\n",
    "            self.states[i] = self._get_state(idx - 1)\n",
    "            self.new_states[i] = self._get_state(idx)\n",
    "                \n",
    "        return np.transpose(self.states, axes=(0,2,3,1)), self.actions[self.indices], self.rewards[self.indices], np.transpose(self.new_states, axes=(0,2,3,1)), self.terminal_flags[self.indices]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(session, replay_memory, main_ddqn, target_ddqn, batch_size, gamma):\n",
    "    states, actions, rewards, new_states, terminal_flags = replay_memory.get_minibatch()\n",
    "    \n",
    "    best_new_actions = session.run(main_ddqn.best_action, feed_dict={main_ddqn.input:new_states})\n",
    "    \n",
    "    q_vals = session.run(target_ddqn.q_values, feed_dict={target_ddqn.input:new_states})\n",
    "    double_q = q_vals[range(batch_size), best_new_actions]\n",
    "    \n",
    "    target_q = rewards + (gamma*double_q*(1 - terminal_flags))\n",
    "    \n",
    "    loss, _= session.run([main_ddqn.loss, main_ddqn.update]\n",
    "                         , feed_dict={main_ddqn.input:states, main_ddqn.target_q:target_q, main_ddqn.action:actions})\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetNetworkUpdater(object):\n",
    "    def __init__(self, main_ddqn_vars, target_ddqn_vars):\n",
    "        self.main_ddqn_vars = main_ddqn_vars\n",
    "        self.target_ddqn_vars = target_ddqn_vars\n",
    "        \n",
    "    def _update_target_vars(self):\n",
    "        update_ops = []\n",
    "        for i, var in enumerate(self.main_ddqn_vars):\n",
    "            copy_op = self.target_ddqn_vars[i].assign(var.value())\n",
    "            update_ops.append(copy_op)\n",
    "        return update_ops\n",
    "    \n",
    "    def __call__(self, sess):\n",
    "        update_ops = self._update_target_vars()\n",
    "        for copy_op in update_ops:\n",
    "            sess.run(copy_op)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreakoutGame(object):\n",
    "    \n",
    "    def __init__(self, env_name=\"BreakoutDeterministic-v4\", no_op_steps=10, history_length=4):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.process_frame = FrameProcessor()\n",
    "        self.state = None\n",
    "        self.last_lives = 0\n",
    "        self.no_op_steps = no_op_steps\n",
    "        self.history_length = history_length\n",
    "        \n",
    "    def reset(self, sess, evaluation=False):\n",
    "        frame = self.env.reset()\n",
    "        self.last_lives = 0\n",
    "        terminal_life_lost = True\n",
    "        \n",
    "        if evaluation:\n",
    "            for _ in range(random.randint(1, self.no_op_steps)):\n",
    "                frame, _, _, _ = self.env.step(1) #fire the ball\n",
    "                \n",
    "        processed_frame = self.process_frame(sess, frame)\n",
    "        self.state = np.repeat(processed_frame, self.history_length, axis=2)\n",
    "        \n",
    "        return terminal_life_lost\n",
    "    \n",
    "    def step(self, sess, action):\n",
    "        \n",
    "        new_frame, reward, terminal, info = self.env.step(action)\n",
    "        \n",
    "        if info['ale.lives'] < self.last_lives:\n",
    "            terminal_life_lost = True\n",
    "        else:\n",
    "            terminal_life_lost = terminal\n",
    "        self.last_lives = info['ale.lives']\n",
    "        \n",
    "        processed_new_frame = self.process_frame(sess, new_frame)\n",
    "        new_state = np.append(self.state[:, :, 1:], processed_new_frame, axis=2)\n",
    "        self.state = new_state\n",
    "        \n",
    "        return processed_new_frame, reward, terminal, terminal_life_lost, new_frame\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip the rewards to be -1, 0, 1. Proved to be benefitial for Breakout\n",
    "def clip_reward(reward):\n",
    "    if reward > 0:\n",
    "        return 1\n",
    "    elif reward == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the hyper-params:\n",
    "\n",
    "MAX_EPISODE_LENGTH = 180\n",
    "EVAL_FREQUENCY = 350000\n",
    "EVAL_STEPS = 100\n",
    "TARGET_NETWORK_UPDATE_FREQ = 1000\n",
    "\n",
    "DISCOUNT_FACTOR = 0.95 # Gamma in the Q learning equation\n",
    "REPLAY_MEMORY_START_SIZE = 500 # random actions at the begining\n",
    "\n",
    "MAX_FRAMES = 70000\n",
    "MEMORY_SIZE = 1000000\n",
    "NO_OP_STEPS = 10\n",
    "\n",
    "UPDATE_FREQ = 4\n",
    "HIDDEN = 512\n",
    "\n",
    "LEARNING_RATE = 0.0000625\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "EXPLORE_START = 1.0\n",
    "EXPLORE_STOP = 0.01\n",
    "DECAY_RATE = 0.00005\n",
    "\n",
    "#IMPORTANT FOR TRAINING OR TESTING:\n",
    "PATH = \"output/\"\n",
    "INFO = \"info\"\n",
    "RUNID = \"run1\"\n",
    "PATH = PATH + RUNID \n",
    "TRAIN = True\n",
    "\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "os.makedirs(os.path.join(INFO, RUNID), exist_ok=True)\n",
    "SUMM_WRITER = tf.summary.FileWriter(os.path.join(INFO, RUNID))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "atari = BreakoutGame()\n",
    "\n",
    "#create main DQN:\n",
    "with tf.variable_scope('mainDDQN'):\n",
    "    MAIN_DDQN = DDQN(atari.env.action_space.n, HIDDEN, LEARNING_RATE)\n",
    "\n",
    "MAIN_DDQN_VARS = tf.trainable_variables(scope='mainDDQN')\n",
    "\n",
    "\n",
    "#create target DQN:\n",
    "with tf.variable_scope('targetDDQN'):\n",
    "    TARGET_DDQN = DDQN(atari.env.action_space.n, HIDDEN)\n",
    "\n",
    "TARGET_DDQN_VARS = tf.trainable_variables(scope='targetDDQN')\n",
    "\n",
    "\n",
    "init= tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS_IDS = [\"conv1\", \"conv2\", \"conv3\", \"value_fc\", \"value\", \"advantage_fc\", \"advantages\"]\n",
    "\n",
    "with tf.name_scope(\"Performace\"):\n",
    "    LOSS_PH = tf.placeholder(tf.float32, shape=None, name=\"loss_summary\")\n",
    "    LOSS_SUMMARY = tf.summary.scalar(\"loss\", LOSS_PH)\n",
    "    REWARD_PH = tf.placeholder(tf.float32, shape=None, name=\"reward_summary\")\n",
    "    REWARD_SUMMARY = tf.summary.scalar(\"reward\", REWARD_PH)\n",
    "    EVAL_SCORE_PH = tf.placeholder(tf.float32, shape=None, name=\"eval_summary\")\n",
    "    EVAL_SCORE_SUMMARY = tf.summary.scalar(\"eval_score\", EVAL_SCORE_PH)\n",
    "    \n",
    "PERFORMANCE_SUMMARIES = tf.summary.merge([LOSS_SUMMARY, REWARD_SUMMARY])\n",
    "\n",
    "with tf.name_scope(\"Parameters\"):\n",
    "    ALL_PARAM_SUMMARIES = []\n",
    "    for i, Id in enumerate(LAYERS_IDS):\n",
    "        with tf.name_scope(\"mainDDQN/\"):\n",
    "            MAIN_DDQN_KERNEL = tf.summary.histogram(Id, tf.reshape(MAIN_DDQN_VARS[i], shape=[-1]))\n",
    "        ALL_PARAM_SUMMARIES.extend([MAIN_DDQN_KERNEL])\n",
    "PARAM_SUMMARIES = tf.summary.merge(ALL_PARAM_SUMMARIES)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    my_replay_memory = ReplayMemory(size=MEMORY_SIZE, batch_size=BATCH_SIZE)\n",
    "    update_networks = TargetNetworkUpdater(MAIN_DDQN_VARS, TARGET_DDQN_VARS)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        frame_number = 0\n",
    "        rewards = []\n",
    "        loss_list = []\n",
    "        \n",
    "        decay_step = 0\n",
    "        tau = 0\n",
    "        \n",
    "        while frame_number < MAX_FRAMES:\n",
    "            print(frame_number)\n",
    "        #training:\n",
    "            epoch_frame = 0\n",
    "            while epoch_frame < EVAL_FREQUENCY:\n",
    "                terminal_life_lost = atari.reset(sess)\n",
    "                episode_reward_sum = 0\n",
    "                for _ in range(MAX_EPISODE_LENGTH):\n",
    "                    \n",
    "                    #take the action:\n",
    "                    action, explore_prob = predict_action(sess, MAIN_DDQN, EXPLORE_START, EXPLORE_STOP,\n",
    "                                                          DECAY_RATE, decay_step, atari.state, action_size=ACTION_SIZE)\n",
    "                    decay_step += 1\n",
    "                    \n",
    "                    processed_new_frame, reward, terminal, terminal_life_lost, _ = atari.step(sess, action)\n",
    "                    \n",
    "                    frame_number += 1\n",
    "                    epoch_frame += 1\n",
    "                    episode_reward_sum += reward\n",
    "                    \n",
    "                    clipped_reward = clip_reward(reward)\n",
    "                    \n",
    "                    my_replay_memory.add_experience(action=action, frame=processed_new_frame[:, :, 0], reward=clipped_reward,\n",
    "                                                   terminal=terminal_life_lost)\n",
    "                    \n",
    "                    if frame_number % UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        loss = learn(sess, my_replay_memory, MAIN_DDQN, TARGET_DDQN, BATCH_SIZE, gamma = DISCOUNT_FACTOR)\n",
    "                        loss_list.append(loss)\n",
    "                    \n",
    "                    if frame_number % TARGET_NETWORK_UPDATE_FREQ == 0 and frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        update_networks(sess)\n",
    "                        \n",
    "                    if terminal:\n",
    "                        terminal = False\n",
    "                        break\n",
    "                \n",
    "                rewards.append(episode_reward_sum)\n",
    "                \n",
    "                if len(rewards) % 10 == 0:\n",
    "                    \n",
    "                    if frame_number > REPLAY_MEMORY_START_SIZE:\n",
    "                        summ = sess.run(PERFORMANCE_SUMMARIES, feed_dict={LOSS_PH:np.mean(loss_list), REWARD_PH:np.mean(rewards[-10:])})\n",
    "                        \n",
    "                        SUMM_WRITER.add_summary(summ, frame_number)\n",
    "                        loss_list = []\n",
    "                        \n",
    "                    summ_param = sess.run(PARAM_SUMMARIES)\n",
    "                    SUMM_WRITER.add_summary(summ_param, frame_number)\n",
    "                    \n",
    "                    print(\"Number of episodes: {}\".format(len(rewards)), \"Frame number: {}\".format(frame_number),\n",
    "                          \"Rewards mean for the last 10 episodes: {}\".format(np.mean(rewards[-10:])))\n",
    "                    with open(\"rewards.dat\", \"a\") as reward_file:\n",
    "                        print(len(rewards), frame_number, np.mean(rewards[-10:]), file=reward_file)\n",
    "            \n",
    "            #evaluation:\n",
    "            \n",
    "            terminal = True\n",
    "            eval_rewards = []\n",
    "            evaluate_frame_number = 0\n",
    "            \n",
    "            for _ in range(EVAL_STEPS):\n",
    "                if terminal:\n",
    "                    terminal_life_lost = atari.reset(sess, evaluation=True)\n",
    "                    episode_reward_sum = 0\n",
    "                    terminal = False\n",
    "                    \n",
    "                if terminal_life_lost:\n",
    "                    action = 1 \n",
    "                else: \n",
    "                    action, _ = predict_action(sess, MAIN_DDQN, EXPLORE_START, EXPLORE_STOP,\n",
    "                                                                         DECAY_RATE, decay_step, atari.state, ACTION_SIZE, evaluation=True)\n",
    "                \n",
    "                processed_new_frame, reward, terminal, terminal_life_lost, _ = atari.step(sess, action)\n",
    "                evaluate_frame_number += 1\n",
    "                episode_reward_sum += reward\n",
    "                \n",
    "                if terminal:\n",
    "                    eval_rewards.append(episode_reward_sum)\n",
    "                \n",
    "            print(\"Evaluation score:\\n\", np.mean(eval_rewards))\n",
    "            \n",
    "            #save network params:\n",
    "            saver.save(sess, PATH+\"/my_model\", global_step=frame_number)\n",
    "            \n",
    "            summ = sess.run(EVAL_SCORE_SUMMARY, feed_dict={EVAL_SCORE_PH:np.mean(eval_rewards)})\n",
    "            SUMM_WRITER.add_summary(summ, frame_number)\n",
    "            \n",
    "            with open(\"rewardsEval.dat\", \"a\") as eval_reward_file:\n",
    "                 print(frame_number, np.mean(eval_rewards), file=eval_reward_file)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Number of episodes: 10 Frame number: 1559 Rewards mean for the last 10 episodes: 0.8\n",
      "Number of episodes: 20 Frame number: 3203 Rewards mean for the last 10 episodes: 1.2\n",
      "Number of episodes: 30 Frame number: 4871 Rewards mean for the last 10 episodes: 1.0\n",
      "Number of episodes: 40 Frame number: 6575 Rewards mean for the last 10 episodes: 1.3\n",
      "Number of episodes: 50 Frame number: 8156 Rewards mean for the last 10 episodes: 0.7\n",
      "Number of episodes: 60 Frame number: 9777 Rewards mean for the last 10 episodes: 0.8\n",
      "Number of episodes: 70 Frame number: 11486 Rewards mean for the last 10 episodes: 1.2\n",
      "Number of episodes: 80 Frame number: 13220 Rewards mean for the last 10 episodes: 1.6\n",
      "Number of episodes: 90 Frame number: 14916 Rewards mean for the last 10 episodes: 1.2\n",
      "Number of episodes: 100 Frame number: 16665 Rewards mean for the last 10 episodes: 1.2\n",
      "Number of episodes: 110 Frame number: 18401 Rewards mean for the last 10 episodes: 1.2\n",
      "Number of episodes: 120 Frame number: 20124 Rewards mean for the last 10 episodes: 1.0\n",
      "Number of episodes: 130 Frame number: 21897 Rewards mean for the last 10 episodes: 1.3\n",
      "Number of episodes: 140 Frame number: 23622 Rewards mean for the last 10 episodes: 1.2\n",
      "Number of episodes: 150 Frame number: 25418 Rewards mean for the last 10 episodes: 1.5\n",
      "Number of episodes: 160 Frame number: 27137 Rewards mean for the last 10 episodes: 1.0\n",
      "Number of episodes: 170 Frame number: 28922 Rewards mean for the last 10 episodes: 1.4\n",
      "Number of episodes: 180 Frame number: 30712 Rewards mean for the last 10 episodes: 1.1\n",
      "Number of episodes: 190 Frame number: 32505 Rewards mean for the last 10 episodes: 0.9\n",
      "Number of episodes: 200 Frame number: 34305 Rewards mean for the last 10 episodes: 1.5\n",
      "Number of episodes: 210 Frame number: 36097 Rewards mean for the last 10 episodes: 1.5\n",
      "Number of episodes: 220 Frame number: 37869 Rewards mean for the last 10 episodes: 1.4\n",
      "Number of episodes: 230 Frame number: 39590 Rewards mean for the last 10 episodes: 0.8\n",
      "Number of episodes: 240 Frame number: 41368 Rewards mean for the last 10 episodes: 1.9\n",
      "Number of episodes: 250 Frame number: 43150 Rewards mean for the last 10 episodes: 1.1\n",
      "Number of episodes: 260 Frame number: 44950 Rewards mean for the last 10 episodes: 0.9\n",
      "Number of episodes: 270 Frame number: 46749 Rewards mean for the last 10 episodes: 1.0\n",
      "Number of episodes: 280 Frame number: 48537 Rewards mean for the last 10 episodes: 0.7\n",
      "Number of episodes: 290 Frame number: 50337 Rewards mean for the last 10 episodes: 0.9\n",
      "Number of episodes: 300 Frame number: 52137 Rewards mean for the last 10 episodes: 1.1\n",
      "Number of episodes: 310 Frame number: 53914 Rewards mean for the last 10 episodes: 1.6\n",
      "Number of episodes: 320 Frame number: 55709 Rewards mean for the last 10 episodes: 1.3\n",
      "Number of episodes: 330 Frame number: 57509 Rewards mean for the last 10 episodes: 1.2\n",
      "Number of episodes: 340 Frame number: 59309 Rewards mean for the last 10 episodes: 1.6\n",
      "Number of episodes: 350 Frame number: 61109 Rewards mean for the last 10 episodes: 1.1\n",
      "Number of episodes: 360 Frame number: 62909 Rewards mean for the last 10 episodes: 1.4\n",
      "Number of episodes: 370 Frame number: 64704 Rewards mean for the last 10 episodes: 1.2\n",
      "Number of episodes: 380 Frame number: 66504 Rewards mean for the last 10 episodes: 1.3\n",
      "Number of episodes: 390 Frame number: 68304 Rewards mean for the last 10 episodes: 1.5\n",
      "Number of episodes: 400 Frame number: 70104 Rewards mean for the last 10 episodes: 0.9\n",
      "Number of episodes: 410 Frame number: 71904 Rewards mean for the last 10 episodes: 0.8\n",
      "Number of episodes: 420 Frame number: 73704 Rewards mean for the last 10 episodes: 0.5\n",
      "Number of episodes: 430 Frame number: 75504 Rewards mean for the last 10 episodes: 0.7\n",
      "Number of episodes: 440 Frame number: 77304 Rewards mean for the last 10 episodes: 1.0\n",
      "Number of episodes: 450 Frame number: 79104 Rewards mean for the last 10 episodes: 0.8\n",
      "Number of episodes: 460 Frame number: 80904 Rewards mean for the last 10 episodes: 1.4\n",
      "Number of episodes: 470 Frame number: 82704 Rewards mean for the last 10 episodes: 0.9\n",
      "Number of episodes: 480 Frame number: 84504 Rewards mean for the last 10 episodes: 0.9\n",
      "Number of episodes: 490 Frame number: 86304 Rewards mean for the last 10 episodes: 1.3\n",
      "Number of episodes: 500 Frame number: 88046 Rewards mean for the last 10 episodes: 0.8\n",
      "Number of episodes: 510 Frame number: 89846 Rewards mean for the last 10 episodes: 0.9\n",
      "Number of episodes: 520 Frame number: 91646 Rewards mean for the last 10 episodes: 1.3\n",
      "Number of episodes: 530 Frame number: 93446 Rewards mean for the last 10 episodes: 1.0\n",
      "Number of episodes: 540 Frame number: 95246 Rewards mean for the last 10 episodes: 1.2\n",
      "Number of episodes: 550 Frame number: 97046 Rewards mean for the last 10 episodes: 1.4\n",
      "Number of episodes: 560 Frame number: 98846 Rewards mean for the last 10 episodes: 1.2\n",
      "Number of episodes: 570 Frame number: 100646 Rewards mean for the last 10 episodes: 1.6\n",
      "Number of episodes: 580 Frame number: 102446 Rewards mean for the last 10 episodes: 0.9\n",
      "Number of episodes: 590 Frame number: 104246 Rewards mean for the last 10 episodes: 0.9\n",
      "Number of episodes: 600 Frame number: 106046 Rewards mean for the last 10 episodes: 1.3\n",
      "Number of episodes: 610 Frame number: 107846 Rewards mean for the last 10 episodes: 0.9\n",
      "Number of episodes: 620 Frame number: 109646 Rewards mean for the last 10 episodes: 0.8\n",
      "Number of episodes: 630 Frame number: 111446 Rewards mean for the last 10 episodes: 1.1\n",
      "Number of episodes: 640 Frame number: 113246 Rewards mean for the last 10 episodes: 0.7\n",
      "Number of episodes: 650 Frame number: 115045 Rewards mean for the last 10 episodes: 0.9\n",
      "Number of episodes: 660 Frame number: 116845 Rewards mean for the last 10 episodes: 1.0\n",
      "Number of episodes: 670 Frame number: 118641 Rewards mean for the last 10 episodes: 1.0\n",
      "Number of episodes: 680 Frame number: 120441 Rewards mean for the last 10 episodes: 1.2\n",
      "Number of episodes: 690 Frame number: 122241 Rewards mean for the last 10 episodes: 1.1\n",
      "Number of episodes: 700 Frame number: 124041 Rewards mean for the last 10 episodes: 1.3\n",
      "Number of episodes: 710 Frame number: 125841 Rewards mean for the last 10 episodes: 1.0\n",
      "Number of episodes: 720 Frame number: 127641 Rewards mean for the last 10 episodes: 1.0\n",
      "Number of episodes: 730 Frame number: 129441 Rewards mean for the last 10 episodes: 0.9\n",
      "Number of episodes: 740 Frame number: 131241 Rewards mean for the last 10 episodes: 1.1\n",
      "Number of episodes: 750 Frame number: 133041 Rewards mean for the last 10 episodes: 0.6\n",
      "Number of episodes: 760 Frame number: 134841 Rewards mean for the last 10 episodes: 1.4\n",
      "Number of episodes: 770 Frame number: 136641 Rewards mean for the last 10 episodes: 1.3\n",
      "Number of episodes: 780 Frame number: 138441 Rewards mean for the last 10 episodes: 1.9\n",
      "Number of episodes: 790 Frame number: 140241 Rewards mean for the last 10 episodes: 1.3\n",
      "Number of episodes: 800 Frame number: 142041 Rewards mean for the last 10 episodes: 1.0\n",
      "Number of episodes: 810 Frame number: 143841 Rewards mean for the last 10 episodes: 0.8\n",
      "Number of episodes: 820 Frame number: 145641 Rewards mean for the last 10 episodes: 1.3\n",
      "Number of episodes: 830 Frame number: 147441 Rewards mean for the last 10 episodes: 0.9\n",
      "Number of episodes: 840 Frame number: 149241 Rewards mean for the last 10 episodes: 1.0\n",
      "Number of episodes: 850 Frame number: 151041 Rewards mean for the last 10 episodes: 1.2\n",
      "Number of episodes: 860 Frame number: 152841 Rewards mean for the last 10 episodes: 1.6\n",
      "Number of episodes: 870 Frame number: 154641 Rewards mean for the last 10 episodes: 1.0\n",
      "Number of episodes: 880 Frame number: 156441 Rewards mean for the last 10 episodes: 1.3\n",
      "Number of episodes: 890 Frame number: 158241 Rewards mean for the last 10 episodes: 1.5\n",
      "Number of episodes: 900 Frame number: 160041 Rewards mean for the last 10 episodes: 0.9\n",
      "Number of episodes: 910 Frame number: 161841 Rewards mean for the last 10 episodes: 0.9\n",
      "Number of episodes: 920 Frame number: 163641 Rewards mean for the last 10 episodes: 1.2\n",
      "Number of episodes: 930 Frame number: 165441 Rewards mean for the last 10 episodes: 0.8\n",
      "Number of episodes: 940 Frame number: 167241 Rewards mean for the last 10 episodes: 1.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes: 950 Frame number: 169041 Rewards mean for the last 10 episodes: 0.8\n",
      "Number of episodes: 960 Frame number: 170841 Rewards mean for the last 10 episodes: 1.2\n",
      "Number of episodes: 970 Frame number: 172641 Rewards mean for the last 10 episodes: 1.2\n",
      "Number of episodes: 980 Frame number: 174441 Rewards mean for the last 10 episodes: 1.2\n",
      "Number of episodes: 990 Frame number: 176241 Rewards mean for the last 10 episodes: 1.3\n",
      "Number of episodes: 1000 Frame number: 178041 Rewards mean for the last 10 episodes: 1.3\n",
      "Number of episodes: 1010 Frame number: 179841 Rewards mean for the last 10 episodes: 1.5\n",
      "Number of episodes: 1020 Frame number: 181641 Rewards mean for the last 10 episodes: 1.1\n",
      "Number of episodes: 1030 Frame number: 183441 Rewards mean for the last 10 episodes: 0.8\n",
      "Number of episodes: 1040 Frame number: 185241 Rewards mean for the last 10 episodes: 0.9\n",
      "Number of episodes: 1050 Frame number: 187041 Rewards mean for the last 10 episodes: 1.6\n",
      "Number of episodes: 1060 Frame number: 188841 Rewards mean for the last 10 episodes: 0.9\n",
      "Number of episodes: 1070 Frame number: 190641 Rewards mean for the last 10 episodes: 0.8\n",
      "Number of episodes: 1080 Frame number: 192441 Rewards mean for the last 10 episodes: 1.4\n",
      "Number of episodes: 1090 Frame number: 194241 Rewards mean for the last 10 episodes: 1.3\n",
      "Number of episodes: 1100 Frame number: 196041 Rewards mean for the last 10 episodes: 0.8\n",
      "Number of episodes: 1110 Frame number: 197841 Rewards mean for the last 10 episodes: 1.3\n",
      "Number of episodes: 1120 Frame number: 199641 Rewards mean for the last 10 episodes: 1.1\n",
      "Number of episodes: 1130 Frame number: 201441 Rewards mean for the last 10 episodes: 1.0\n",
      "Number of episodes: 1140 Frame number: 203241 Rewards mean for the last 10 episodes: 1.0\n",
      "Number of episodes: 1150 Frame number: 205041 Rewards mean for the last 10 episodes: 0.8\n",
      "Number of episodes: 1160 Frame number: 206841 Rewards mean for the last 10 episodes: 0.9\n",
      "Number of episodes: 1170 Frame number: 208641 Rewards mean for the last 10 episodes: 1.7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-831e35bdf792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-ebd0dc0d8ba8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mframe_number\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mUPDATE_FREQ\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mframe_number\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mREPLAY_MEMORY_START_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_replay_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAIN_DDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTARGET_DDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDISCOUNT_FACTOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                         \u001b[0mloss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f80c3a35da45>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(session, replay_memory, main_ddqn, target_ddqn, batch_size, gamma)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     loss, _= session.run([main_ddqn.loss, main_ddqn.update]\n\u001b[0;32m---> 12\u001b[0;31m                          , feed_dict={main_ddqn.input:states, main_ddqn.target_q:target_q, main_ddqn.action:actions})\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/final_project/final_project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/final_project/final_project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/final_project/final_project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/final_project/final_project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/final_project/final_project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/final_project/final_project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
